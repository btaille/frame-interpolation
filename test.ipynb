{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achitecture 1 : \"Learning Image Matching by Simply Watching Video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_conv_layers = [6,96,96,128,128,128]\n",
    "_activation = 'relu'\n",
    "\n",
    "_batch_size = 16 #16 in the paper but too expensive for the GPU\n",
    "_learning_rate = 1e-5\n",
    "_epochs = 20\n",
    "_step_test = 50\n",
    "_step_viz = 50\n",
    "\n",
    "_dataset = \"KITTI\"\n",
    "# _dataset = \"SINTEL\"\n",
    "\n",
    "test_dataset = \"SINTEL\"\n",
    "\n",
    "if test_dataset == \"KITTI\":\n",
    "    _h,_w = (128,384)\n",
    "if test_dataset == \"SINTEL\":\n",
    "    _h,_w = (128,256)\n",
    "\n",
    "_data_folder = \"data/%s\"%_dataset\n",
    "_train_folder = \"train/%s\"%_dataset\n",
    "_test_folder = \"test/%s\"%_dataset\n",
    "_previously_trained = True\n",
    "\n",
    "\n",
    "if not os.path.exists(_test_folder):\n",
    "    os.makedirs(_test_folder)\n",
    "    print(\"Directory created :\", _test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x,name=None):\n",
    "    if _activation == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x,name)\n",
    "    if _activation == 'relu':\n",
    "        return tf.nn.relu(x,name)\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_full_sequence_batch(dataset, seq, train = 1, folder=0, quality =\"clean\"):    \n",
    "    if dataset == \"SINTEL\":\n",
    "        if train:\n",
    "            folder = \"train\"\n",
    "        else:\n",
    "            folder = \"test\"\n",
    "            \n",
    "        l = len(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq))-2\n",
    "        frames = np.ndarray([3,l,128,256, 3],np.float32)\n",
    "        for i in range(l):\n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq+\"/frame_%s\"%'{0:04}'.format(i)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq+\"/frame_%s\"%'{0:04}'.format(i+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq+\"/frame_%s\"%'{0:04}'.format(i+2)+\".png\")\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        l = len(os.listdir(\"data/\"+_dataset+\"/kitti_resized/\"+folder+\"/\"+seq))-2\n",
    "        frames = np.ndarray([3,l,128,384, 3],np.float32)\n",
    "        \n",
    "        for i in range(l):  \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder+\"/\"+seq+\"/%s\"%'{0:010}'.format(i)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder+\"/\"+seq+\"/%s\"%'{0:010}'.format(i+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder+\"/\"+seq+\"/%s\"%'{0:010}'.format(i+2)+\".png\")\n",
    "    \n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n",
    "\n",
    "\n",
    "def generate_batch(dataset,batch_size, train = 1, quality =\"clean\"):\n",
    "    \n",
    "    if dataset == \"SINTEL\":\n",
    "        frames = np.ndarray([3,batch_size,128,256, 3],np.float32)\n",
    "        if train:\n",
    "            folder = \"train\"\n",
    "        else:\n",
    "            folder = \"test\"\n",
    "        \n",
    "        seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality),batch_size)        \n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+2)+\".png\")\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]))\n",
    "            \n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n",
    "\n",
    "def load_batch(dataset,batch_size, train = 0):\n",
    "    ground_truth = cv2.imread(_train_folder+\"/ground_truth.png\")  \n",
    "    \n",
    "    ground_truth= np.split(ground_truth,batch_size,axis=0)\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for seq in os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]):\n",
    "                for index in range(len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-1):\n",
    "                    if (cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\") == ground_truth[i]).all():\n",
    "                    \n",
    "                        frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "                        frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "                        frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(\"float32\",[None,_h,_w,6],name='x_input')\n",
    "x_gt = tf.placeholder(\"float32\",[None,_h,_w,3],name='x_ground-truth')\n",
    "\n",
    "#CONVOLUTIONAL ENCODER\n",
    "\n",
    "with tf.variable_scope(\"CONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W1_1 = weight_variable([3,3,_conv_layers[0],_conv_layers[1]])\n",
    "        b1_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti1_1 = tf.nn.bias_add(tf.nn.conv2d(x,W1_1,[1,1,1,1],padding='SAME'),b1_1)\n",
    "        conv1_1 = activation(preacti1_1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W2_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b2_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti2_1 = tf.nn.bias_add(tf.nn.conv2d(conv1_1,W2_1,[1,1,1,1],padding='SAME'),b2_1)\n",
    "        conv2_1 = activation(preacti2_1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        W3_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b3_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti3_1 = tf.nn.bias_add(tf.nn.conv2d(conv2_1,W3_1,[1,1,1,1],padding='SAME'),b3_1)\n",
    "        conv3_1 = activation(preacti3_1,name=scope.name)\n",
    "        \n",
    "    pool_1 = tf.nn.max_pool(conv3_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(2,6):    \n",
    "    with tf.variable_scope(\"CONV-BLOCK%s\"%i) as scope:\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            globals()['W1_%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i]])\n",
    "            globals()['b1_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti1_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['pool_%s'%(i-1)],globals()['W1_%s'%i],[1,1,1,1],padding='SAME'),globals()['b1_%s'%i])\n",
    "            globals()['conv1_%s'%i] = activation(globals()['preacti1_%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            globals()['W2_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b2_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti2_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv1_%s'%i],globals()['W2_%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_%s'%i])\n",
    "            globals()['conv2_%s'%i] = activation(globals()['preacti2_%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            globals()['W3_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b3_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti3_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv2_%s'%i],globals()['W3_%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_%s'%i])\n",
    "            globals()['conv3_%s'%i] = activation(globals()['preacti3_%s'%i],name=scope.name)\n",
    "\n",
    "        globals()['pool_%s'%i] = tf.nn.max_pool(globals()['conv3_%s'%i], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "\n",
    "#DECONVOLUTIONAL DECODER\n",
    "\n",
    "batch_size = tf.shape(x)[0]\n",
    "\n",
    "with tf.variable_scope(\"DECONV-BLOCK5\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d5 = weight_variable([4,4,_conv_layers[5],_conv_layers[4]])\n",
    "        b1_d5 = bias_variable([_conv_layers[4]])\n",
    "        convtr_d5 = tf.nn.conv2d_transpose(pool_5,W1_d5,[batch_size,int(_h/2**4),int(_w/2**4),_conv_layers[4]],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d5 = tf.nn.bias_add(convtr_d5,b1_d5)\n",
    "        conv1_d5 = activation(preacti1_d5,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b2_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti2_d5 = tf.nn.bias_add(tf.nn.conv2d(conv1_d5,W2_d5,[1,1,1,1],padding='SAME'),b2_d5)\n",
    "        conv2_d5 = activation(preacti2_d5,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b3_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti3_d5 = tf.nn.bias_add(tf.nn.conv2d(conv2_d5,W3_d5,[1,1,1,1],padding='SAME'),b3_d5)\n",
    "        conv3_d5 = activation(preacti3_d5,name=scope.name)\n",
    "\n",
    "        \n",
    "for i in range(4,1,-1):\n",
    "    with tf.variable_scope(\"DECONV-BLOCK%s\"%i) as scope:\n",
    "        \n",
    "        globals()['concat_d%s'%i] = tf.concat(3,[globals()['conv3_d%s'%(i+1)],globals()['pool_%s'%(i)]])\n",
    "        \n",
    "        with tf.variable_scope('deconv1') as scope:\n",
    "            globals()['W1_d%s'%i] = weight_variable([4,4,_conv_layers[i-1],2*_conv_layers[i]])\n",
    "            globals()['b1_d%s'%i] = bias_variable([_conv_layers[i-1]])\n",
    "            globals()['convtr_d%s'%i] = tf.nn.conv2d_transpose(globals()['concat_d%s'%i],globals()['W1_d%s'%i],[batch_size,int(_h/2**(i-1)),int(_w/2**(i-1)),_conv_layers[i-1]],[1,2,2,1],padding='SAME')\n",
    "            globals()['preacti1_d%s'%i] = tf.nn.bias_add(globals()['convtr_d%s'%i],globals()['b1_d%s'%i])\n",
    "            globals()['conv1_d%s'%i] = activation(globals()['preacti1_d%s'%i],name=scope.name)\n",
    "            \n",
    "        with tf.variable_scope('deconv2') as scope:\n",
    "            globals()['W2_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b2_d%s'%i] = bias_variable([_conv_layers[i-1]])  \n",
    "            globals()['preacti2_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv1_d%s'%i],globals()['W2_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_d%s'%i])\n",
    "            globals()['conv2_d%s'%i] = activation(globals()['preacti2_d%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('deconv3') as scope:\n",
    "            globals()['W3_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b3_d%s'%i] = bias_variable([_conv_layers[i-1]])             \n",
    "            globals()['preacti3_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv2_d%s'%i],globals()['W3_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_d%s'%i])\n",
    "            globals()['conv3_d%s'%i] = activation(globals()['preacti3_d%s'%i],name=scope.name)\n",
    "            \n",
    "            \n",
    "with tf.variable_scope(\"DECONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d1 = weight_variable([4,4,3,_conv_layers[1]])\n",
    "        b1_d1 = bias_variable([3])\n",
    "        convtr_d1 = tf.nn.conv2d_transpose(conv3_d2,W1_d1,[batch_size,_h,_w,3],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d1 = tf.nn.bias_add(convtr_d1,b1_d1)\n",
    "        conv1_d1 = activation(preacti1_d1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d1 = weight_variable([3,3,3,3])\n",
    "        b2_d1 = bias_variable([3])             \n",
    "        preacti2_d1 = tf.nn.bias_add(tf.nn.conv2d(conv1_d1,W2_d1,[1,1,1,1],padding='SAME'),b2_d1)\n",
    "        conv2_d1 = activation(preacti2_d1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d1 = weight_variable([3,3,3,3])\n",
    "        b3_d1 = bias_variable([3])             \n",
    "        preacti3_d1 = tf.nn.bias_add(tf.nn.conv2d(conv2_d1,W3_d1,[1,1,1,1],padding='SAME'),b3_d1)\n",
    "        conv3_d1 = activation(preacti3_d1,name=scope.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential improvements : add dropout and batch normalization (not in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charbonnier_loss(x):\n",
    "    return tf.sqrt(0.1**2+tf.reduce_mean(tf.square(x)))\n",
    "    \n",
    "def interpol_error(x,gt):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x,gt))))\n",
    "\n",
    "def normalized_interpol_error(x,gt):\n",
    "    norm_grad = cv2.Sobel(gt,cv2.CV_64F,1,0,ksize=5)**2+cv2.Sobel(gt,cv2.CV_64F,0,1,ksize=5)**2 \n",
    "    return tf.sqrt(tf.reduce_mean(tf.div(tf.square(tf.sub(x,gt)),(norm_grad+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    cost = charbonnier_loss(tf.sub(conv3_d1,x_gt))\n",
    "    tf.summary.scalar('loss',cost)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#launch graph\n",
    "sess.run(init)\n",
    "\n",
    "# restore weights from model\n",
    "saver.restore(sess, _train_folder+\"/model.ckpt\")\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if test_dataset == \"KITTI\"\n",
    "    #test on kitti\n",
    "    mini_batch_size = 10\n",
    "\n",
    "    start = time.time()\n",
    "    folder = \"2011_09_29\"\n",
    "    for seq in os.listdir(\"data/KITTI/kitti_resized/\"+folder):\n",
    "        batch = generate_full_sequence_batch(\"KITTI\", seq, train = 0, folder = folder)\n",
    "\n",
    "        index = 0\n",
    "        while index < batch[0].shape[0]:\n",
    "            mini_batch = [batch[0][index:index+mini_batch_size],batch[1][index:index+mini_batch_size]]\n",
    "\n",
    "            feed = {x: mini_batch[0], x_gt: mini_batch[1]}        \n",
    "            x_reconstruct,c =  sess.run([conv3_d1,cost], feed_dict=feed)\n",
    "\n",
    "            if not os.path.exists(_test_folder+\"/\"+folder+\"/\"+seq):\n",
    "                os.makedirs(_test_folder+\"/\"+folder+\"/\"+seq)\n",
    "                print(\"Directory created :\",_test_folder+\"/\"+folder+\"/\"+seq)\n",
    "\n",
    "            for i in range(x_reconstruct.shape[0]):\n",
    "                cv2.imwrite(_test_folder+\"/\"+folder+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+i+1)+\".png\",255*x_reconstruct[i])\n",
    "\n",
    "            print(\"Sequence %s : cost %s , time %s\"%(seq,c,time.time()-start))\n",
    "\n",
    "            index += mini_batch_size\n",
    "\n",
    "    print(\"Test Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created : test/KITTI/ambush_1\n",
      "Sequence ambush_1 : cost 0.203717 , time 3.5373122692108154\n",
      "Sequence ambush_1 : cost 0.157576 , time 3.965120315551758\n",
      "Sequence ambush_1 : cost 0.125225 , time 5.579365015029907\n",
      "Directory created : test/KITTI/ambush_3\n",
      "Sequence ambush_3 : cost 0.175527 , time 6.178162336349487\n",
      "Sequence ambush_3 : cost 0.133414 , time 6.618901252746582\n",
      "Sequence ambush_3 : cost 0.153471 , time 7.063467741012573\n",
      "Sequence ambush_3 : cost 0.140891 , time 10.005988836288452\n",
      "Directory created : test/KITTI/bamboo_3\n",
      "Sequence bamboo_3 : cost 0.176403 , time 10.660010814666748\n",
      "Sequence bamboo_3 : cost 0.145766 , time 11.093683004379272\n",
      "Sequence bamboo_3 : cost 0.157063 , time 11.522085905075073\n",
      "Sequence bamboo_3 : cost 0.154091 , time 11.992233514785767\n",
      "Sequence bamboo_3 : cost 0.15722 , time 14.497493982315063\n",
      "Directory created : test/KITTI/cave_3\n",
      "Sequence cave_3 : cost 0.12234 , time 15.137131214141846\n",
      "Sequence cave_3 : cost 0.119831 , time 15.562030553817749\n",
      "Sequence cave_3 : cost 0.151817 , time 15.987217426300049\n",
      "Sequence cave_3 : cost 0.144677 , time 16.415285110473633\n",
      "Sequence cave_3 : cost 0.124282 , time 16.76827597618103\n",
      "Directory created : test/KITTI/market_1\n",
      "Sequence market_1 : cost 0.139187 , time 17.41347312927246\n",
      "Sequence market_1 : cost 0.151398 , time 17.837392330169678\n",
      "Sequence market_1 : cost 0.129028 , time 18.273517847061157\n",
      "Sequence market_1 : cost 0.123638 , time 18.728612184524536\n",
      "Sequence market_1 : cost 0.118756 , time 19.08707356452942\n",
      "Directory created : test/KITTI/market_4\n",
      "Sequence market_4 : cost 0.168866 , time 19.738157749176025\n",
      "Sequence market_4 : cost 0.134587 , time 20.177139043807983\n",
      "Sequence market_4 : cost 0.150193 , time 20.606709718704224\n",
      "Sequence market_4 : cost 0.135633 , time 21.0942599773407\n",
      "Sequence market_4 : cost 0.158909 , time 21.5209698677063\n",
      "Directory created : test/KITTI/mountain_2\n",
      "Sequence mountain_2 : cost 0.190346 , time 22.22657608985901\n",
      "Sequence mountain_2 : cost 0.10344 , time 22.66329002380371\n",
      "Sequence mountain_2 : cost 0.103597 , time 23.11337637901306\n",
      "Sequence mountain_2 : cost 0.103373 , time 23.605737924575806\n",
      "Sequence mountain_2 : cost 0.103264 , time 24.023832082748413\n",
      "Directory created : test/KITTI/PERTURBED_market_3\n",
      "Sequence PERTURBED_market_3 : cost 0.144802 , time 24.87863278388977\n",
      "Sequence PERTURBED_market_3 : cost 0.11542 , time 25.3337721824646\n",
      "Sequence PERTURBED_market_3 : cost 0.115319 , time 25.816457986831665\n",
      "Sequence PERTURBED_market_3 : cost 0.122352 , time 26.283890962600708\n",
      "Sequence PERTURBED_market_3 : cost 0.120686 , time 26.634108543395996\n",
      "Directory created : test/KITTI/PERTURBED_shaman_1\n",
      "Sequence PERTURBED_shaman_1 : cost 0.113936 , time 27.351731300354004\n",
      "Sequence PERTURBED_shaman_1 : cost 0.104694 , time 27.84130024909973\n",
      "Sequence PERTURBED_shaman_1 : cost 0.105497 , time 28.35167670249939\n",
      "Sequence PERTURBED_shaman_1 : cost 0.105643 , time 28.87345838546753\n",
      "Sequence PERTURBED_shaman_1 : cost 0.104327 , time 29.268428802490234\n",
      "Directory created : test/KITTI/temple_1\n",
      "Sequence temple_1 : cost 0.184121 , time 30.065746545791626\n",
      "Sequence temple_1 : cost 0.139917 , time 30.523627996444702\n",
      "Sequence temple_1 : cost 0.139126 , time 30.96420454978943\n",
      "Sequence temple_1 : cost 0.136667 , time 31.40506649017334\n",
      "Sequence temple_1 : cost 0.138712 , time 31.77736806869507\n",
      "Directory created : test/KITTI/tiger\n",
      "Sequence tiger : cost 0.1598 , time 32.46093678474426\n",
      "Sequence tiger : cost 0.121091 , time 32.92425775527954\n",
      "Sequence tiger : cost 0.126483 , time 33.38279414176941\n",
      "Sequence tiger : cost 0.12717 , time 33.86105990409851\n",
      "Sequence tiger : cost 0.135767 , time 34.234580278396606\n",
      "Directory created : test/KITTI/wall\n",
      "Sequence wall : cost 0.10772 , time 34.86060547828674\n",
      "Sequence wall : cost 0.104778 , time 35.312668323516846\n",
      "Sequence wall : cost 0.104671 , time 35.80387568473816\n",
      "Sequence wall : cost 0.105003 , time 36.24239110946655\n",
      "Sequence wall : cost 0.105528 , time 36.597113847732544\n",
      "Test Finished\n"
     ]
    }
   ],
   "source": [
    "if test_dataset == \"SINTEL\"\n",
    "    #test on sintel\n",
    "    mini_batch_size = 10\n",
    "\n",
    "    start = time.time()\n",
    "    for seq in os.listdir(\"data/SINTEL/test/clean/\"):\n",
    "        batch = generate_full_sequence_batch(\"SINTEL\", seq, train = 0)\n",
    "        index = 0\n",
    "        while index < batch[0].shape[0]:\n",
    "            mini_batch = [batch[0][index:index+mini_batch_size],batch[1][index:index+mini_batch_size]]\n",
    "\n",
    "            feed = {x: mini_batch[0], x_gt: mini_batch[1]}        \n",
    "            x_reconstruct,c =  sess.run([conv3_d1,cost], feed_dict=feed)\n",
    "\n",
    "            if not os.path.exists(_test_folder+\"/\"+seq):\n",
    "                os.makedirs(_test_folder+\"/\"+seq)\n",
    "                print(\"Directory created :\",_test_folder+\"/\"+seq)\n",
    "\n",
    "            for i in range(x_reconstruct.shape[0]):\n",
    "                cv2.imwrite(\"test/SINTEL/\"+seq+\"/%s\"%'{0:010}'.format(index+i+1)+\".png\",255*x_reconstruct[i])\n",
    "\n",
    "            print(\"Sequence %s : cost %s , time %s\"%(seq,c,time.time()-start))\n",
    "\n",
    "            index += mini_batch_size\n",
    "\n",
    "    print(\"Test Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
