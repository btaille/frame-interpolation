{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achitecture 1 : \"Learning Image Matching by Simply Watching Video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_conv_layers = [6,96,96,128,128,128]\n",
    "_activation = 'relu'\n",
    "\n",
    "_batch_size = 4 #16 in the paper but too expensive for the GPU\n",
    "_learning_rate = 5e-4\n",
    "_epochs = 20\n",
    "_step_test = 50\n",
    "_step_viz = 50\n",
    "\n",
    "_dataset = \"KITTI\"\n",
    "# _dataset = \"SINTEL\"\n",
    "\n",
    "if _dataset == \"KITTI\":\n",
    "    _h,_w = (128,384)\n",
    "if _dataset == \"SINTEL\":\n",
    "    _h,_w = (128,256)\n",
    "\n",
    "_data_folder = \"data/%s\"%_dataset\n",
    "_train_folder = \"train/%s\"%_dataset\n",
    "_previously_trained = False\n",
    "\n",
    "if not os.path.exists(_train_folder):\n",
    "    os.makedirs(_train_folder)\n",
    "    print(\"Directory created :\", _train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x,name=None):\n",
    "    if _activation == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x,name)\n",
    "    if _activation == 'relu':\n",
    "        return tf.nn.relu(x,name)\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(dataset,batch_size, train = 1, quality =\"clean\"):\n",
    "    \n",
    "    if dataset == \"SINTEL\":\n",
    "        frames = np.ndarray([3,batch_size,128,256, 3],np.float32)\n",
    "        if train:\n",
    "            folder = \"train\"\n",
    "        else:\n",
    "            folder = \"test\"\n",
    "        \n",
    "        seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality),batch_size)        \n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+2)+\".png\")\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]))\n",
    "            \n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n",
    "\n",
    "def load_batch(dataset,batch_size, train = 0):\n",
    "    ground_truth = cv2.imread(_train_folder+\"/ground_truth.png\")  \n",
    "    \n",
    "    ground_truth= np.split(ground_truth,batch_size,axis=0)\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for seq in os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]):\n",
    "                for index in range(len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-1):\n",
    "                    if (cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\") == ground_truth[i]).all():\n",
    "                    \n",
    "                        frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "                        frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "                        frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(\"float32\",[None,_h,_w,6],name='x_input')\n",
    "x_gt = tf.placeholder(\"float32\",[None,_h,_w,3],name='x_ground-truth')\n",
    "\n",
    "train = tf.constant(True)\n",
    "drop = tf.placeholder(\"float\")\n",
    "\n",
    "#CONVOLUTIONAL ENCODER\n",
    "\n",
    "with tf.variable_scope(\"CONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W1_1 = weight_variable([3,3,_conv_layers[0],_conv_layers[1]])\n",
    "        b1_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti1_1 = tf.nn.bias_add(tf.nn.conv2d(x,W1_1,[1,1,1,1],padding='SAME'),b1_1)        \n",
    "        bn1_1 = batch_norm(preacti1_1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv1_1 = activation(bn1_1,name=scope.name)\n",
    "        drop1_1 = tf.nn.dropout(conv1_1,1-drop)\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W2_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b2_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti2_1 = tf.nn.bias_add(tf.nn.conv2d(drop1_1,W2_1,[1,1,1,1],padding='SAME'),b2_1)\n",
    "        bn2_1 = batch_norm(preacti2_1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv2_1 = activation(bn2_1,name=scope.name)\n",
    "        drop2_1 = tf.nn.dropout(conv2_1,1-drop)\n",
    "        \n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        W3_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b3_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti3_1 = tf.nn.bias_add(tf.nn.conv2d(drop2_1,W3_1,[1,1,1,1],padding='SAME'),b3_1)\n",
    "        bn3_1 = batch_norm(preacti3_1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv3_1 = activation(bn3_1,name=scope.name)\n",
    "        drop3_1 = tf.nn.dropout(conv3_1,1-drop)\n",
    "        \n",
    "    pool_1 = tf.nn.max_pool(drop3_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(2,6):    \n",
    "    with tf.variable_scope(\"CONV-BLOCK%s\"%i) as scope:\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            globals()['W1_%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i]])\n",
    "            globals()['b1_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti1_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['pool_%s'%(i-1)],globals()['W1_%s'%i],[1,1,1,1],padding='SAME'),globals()['b1_%s'%i])\n",
    "            globals()['bn1_%s'%i] = batch_norm(globals()['preacti1_%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv1_%s'%i] = activation(globals()['bn1_%s'%i],name=scope.name)\n",
    "            globals()['drop1_%s'%i] = tf.nn.dropout(globals()['conv1_%s'%i],1-drop)\n",
    "            \n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            globals()['W2_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b2_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti2_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop1_%s'%i],globals()['W2_%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_%s'%i])\n",
    "            globals()['bn2_%s'%i] = batch_norm(globals()['preacti2_%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv2_%s'%i] = activation(globals()['bn2_%s'%i],name=scope.name)\n",
    "            globals()['drop2_%s'%i] = tf.nn.dropout(globals()['conv2_%s'%i],1-drop)\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            globals()['W3_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b3_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti3_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop2_%s'%i],globals()['W3_%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_%s'%i])\n",
    "            globals()['bn3_%s'%i] = batch_norm(globals()['preacti3_%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv3_%s'%i] = activation(globals()['bn3_%s'%i],name=scope.name)\n",
    "            globals()['drop3_%s'%i] = tf.nn.dropout(globals()['conv3_%s'%i],1-drop)\n",
    "\n",
    "        globals()['pool_%s'%i] = tf.nn.max_pool(globals()['drop3_%s'%i], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "\n",
    "#DECONVOLUTIONAL DECODER\n",
    "\n",
    "batch_size = tf.shape(x)[0]\n",
    "\n",
    "with tf.variable_scope(\"DECONV-BLOCK5\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d5 = weight_variable([4,4,_conv_layers[5],_conv_layers[4]])\n",
    "        b1_d5 = bias_variable([_conv_layers[4]])\n",
    "        convtr_d5 = tf.nn.conv2d_transpose(pool_5,W1_d5,[batch_size,int(_h/2**4),int(_w/2**4),_conv_layers[4]],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d5 = tf.nn.bias_add(convtr_d5,b1_d5)\n",
    "        bn1_d5 = batch_norm(preacti1_d5,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv1_d5 = activation(bn1_d5,name=scope.name)\n",
    "        drop1_d5 = tf.nn.dropout(conv1_d5,1-drop)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b2_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti2_d5 = tf.nn.bias_add(tf.nn.conv2d(drop1_d5,W2_d5,[1,1,1,1],padding='SAME'),b2_d5)\n",
    "        bn2_d5 = batch_norm(preacti2_d5,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv2_d5 = activation(bn2_d5,name=scope.name)\n",
    "        drop2_d5 = tf.nn.dropout(conv2_d5,1-drop)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b3_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti3_d5 = tf.nn.bias_add(tf.nn.conv2d(drop2_d5,W3_d5,[1,1,1,1],padding='SAME'),b3_d5)\n",
    "        bn3_d5 = batch_norm(preacti3_d5,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv3_d5 = activation(bn3_d5,name=scope.name)\n",
    "        drop3_d5 = tf.nn.dropout(conv3_d5,1-drop)\n",
    "\n",
    "        \n",
    "for i in range(4,1,-1):\n",
    "    with tf.variable_scope(\"DECONV-BLOCK%s\"%i) as scope:\n",
    "        \n",
    "        globals()['concat_d%s'%i] = tf.concat(3,[globals()['drop3_d%s'%(i+1)],globals()['pool_%s'%(i)]])\n",
    "        \n",
    "        with tf.variable_scope('deconv1') as scope:\n",
    "            globals()['W1_d%s'%i] = weight_variable([4,4,_conv_layers[i-1],2*_conv_layers[i]])\n",
    "            globals()['b1_d%s'%i] = bias_variable([_conv_layers[i-1]])\n",
    "            globals()['convtr_d%s'%i] = tf.nn.conv2d_transpose(globals()['concat_d%s'%i],globals()['W1_d%s'%i],[batch_size,int(_h/2**(i-1)),int(_w/2**(i-1)),_conv_layers[i-1]],[1,2,2,1],padding='SAME')\n",
    "            globals()['preacti1_d%s'%i] = tf.nn.bias_add(globals()['convtr_d%s'%i],globals()['b1_d%s'%i])\n",
    "            globals()['bn1_d%s'%i] = batch_norm(globals()['preacti1_d%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv1_d%s'%i] = activation(globals()['bn1_d%s'%i],name=scope.name)\n",
    "            globals()['drop1_d%s'%i] = tf.nn.dropout(globals()['conv1_d%s'%i],1-drop)\n",
    "            \n",
    "        with tf.variable_scope('deconv2') as scope:\n",
    "            globals()['W2_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b2_d%s'%i] = bias_variable([_conv_layers[i-1]])  \n",
    "            globals()['preacti2_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop1_d%s'%i],globals()['W2_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_d%s'%i])\n",
    "            globals()['bn2_d%s'%i] = batch_norm(globals()['preacti2_d%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv2_d%s'%i] = activation(globals()['bn2_d%s'%i],name=scope.name)\n",
    "            globals()['drop2_d%s'%i] = tf.nn.dropout(globals()['conv2_d%s'%i],1-drop)\n",
    "            \n",
    "        with tf.variable_scope('deconv3') as scope:\n",
    "            globals()['W3_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b3_d%s'%i] = bias_variable([_conv_layers[i-1]])             \n",
    "            globals()['preacti3_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop2_d%s'%i],globals()['W3_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_d%s'%i])\n",
    "            globals()['bn3_d%s'%i] = batch_norm(globals()['preacti3_d%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv3_d%s'%i] = activation(globals()['bn3_d%s'%i],name=scope.name)\n",
    "            globals()['drop3_d%s'%i] = tf.nn.dropout(globals()['conv3_d%s'%i],1-drop)\n",
    "            \n",
    "with tf.variable_scope(\"DECONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d1 = weight_variable([4,4,3,_conv_layers[1]])\n",
    "        b1_d1 = bias_variable([3])\n",
    "        convtr_d1 = tf.nn.conv2d_transpose(conv3_d2,W1_d1,[batch_size,_h,_w,3],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d1 = tf.nn.bias_add(convtr_d1,b1_d1)\n",
    "        bn1_d1 = batch_norm(preacti1_d1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv1_d1 = activation(bn1_d1,name=scope.name)\n",
    "        drop1_d1 = tf.nn.dropout(conv1_d1,1-drop)\n",
    "        \n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d1 = weight_variable([3,3,3,3])\n",
    "        b2_d1 = bias_variable([3])             \n",
    "        preacti2_d1 = tf.nn.bias_add(tf.nn.conv2d(conv1_d1,W2_d1,[1,1,1,1],padding='SAME'),b2_d1)\n",
    "        bn2_d1 = batch_norm(preacti2_d1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv2_d1 = activation(bn2_d1,name=scope.name)\n",
    "        drop2_d1 = tf.nn.dropout(conv2_d1,1-drop)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d1 = weight_variable([3,3,3,3])\n",
    "        b3_d1 = bias_variable([3])             \n",
    "        preacti3_d1 = tf.nn.bias_add(tf.nn.conv2d(conv2_d1,W3_d1,[1,1,1,1],padding='SAME'),b3_d1)\n",
    "        bn3_d1 = batch_norm(preacti3_d1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv3_d1 = activation(bn3_d1,name=scope.name)\n",
    "        drop3_d1 = tf.nn.dropout(conv3_d1,1-drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added batch norm and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charbonnier_loss(x):\n",
    "    return tf.sqrt(0.1**2+tf.reduce_mean(tf.square(x)))\n",
    "    \n",
    "def interpol_error(x,gt):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x,gt))))\n",
    "\n",
    "def normalized_interpol_error(x,gt):\n",
    "    norm_grad = cv2.Sobel(gt,cv2.CV_64F,1,0,ksize=5)**2+cv2.Sobel(gt,cv2.CV_64F,0,1,ksize=5)**2 \n",
    "    return tf.sqrt(tf.reduce_mean(tf.div(tf.square(tf.sub(x,gt)),(norm_grad+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    cost = charbonnier_loss(tf.sub(conv3_d1,x_gt))\n",
    "    tf.summary.scalar('loss',cost)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(_learning_rate, global_step,10000, 0.99, staircase=True)\n",
    "tf.summary.scalar('learning_rate',learning_rate)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(_train_folder+'/cnn', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#launch graph\n",
    "sess.run(init)\n",
    "\n",
    "# restore weights from model\n",
    "if _previously_trained :\n",
    "    saver.restore(sess, _train_folder+\"/model.ckpt\")\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_frames = generate_batch(_dataset,4,train = 0)\n",
    "cv2.imwrite(_train_folder+\"/ground_truth.png\",np.concatenate(255*test_frames[1],0))\n",
    "\n",
    "# test_frames = load_batch(_dataset,8,train=0)\n",
    "# cv2.imwrite(_train_folder+\"/ground_truth2.png\",np.concatenate(255*test_frames[1],0))\n",
    "\n",
    "def test(i):\n",
    "    x_reconstruct =  sess.run(conv3_d1, feed_dict={x: test_frames[0], x_gt:test_frames[1], drop:0})   \n",
    "    \n",
    "#     cv2.namedWindow('reconstruction',flags= cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('reconstruction',np.concatenate(x_reconstruct,0))\n",
    "        \n",
    "#     cv2.namedWindow('ground-truth',flags= cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('ground-truth',np.concatenate(test_frames[1],0))\n",
    "    \n",
    "    cv2.imwrite(_train_folder+\"/reconstr_%s\"%'{0:04}'.format(int(i))+\".png\",255*np.concatenate(x_reconstruct,0))\n",
    "    cv2.imwrite(_train_folder+\"/reconstr.png\",255*np.concatenate(x_reconstruct,0))       \n",
    "#     cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 50: LOSS_BATCH = 0.285415, TIME 105.607278\n",
      "STEP 100: LOSS_BATCH = 0.182938, TIME 218.396536\n",
      "STEP 150: LOSS_BATCH = 0.169087, TIME 331.675244\n",
      "STEP 200: LOSS_BATCH = 0.335788, TIME 450.604728\n",
      "STEP 250: LOSS_BATCH = 0.335377, TIME 560.960822\n",
      "STEP 300: LOSS_BATCH = 0.321129, TIME 674.635555\n",
      "STEP 350: LOSS_BATCH = 0.321896, TIME 792.970636\n",
      "STEP 400: LOSS_BATCH = 0.326711, TIME 908.120253\n",
      "STEP 450: LOSS_BATCH = 0.318600, TIME 1025.490101\n",
      "STEP 500: LOSS_BATCH = 0.335214, TIME 1139.635706\n",
      "STEP 550: LOSS_BATCH = 0.337364, TIME 1260.703253\n",
      "STEP 600: LOSS_BATCH = 0.337279, TIME 1373.209949\n",
      "STEP 650: LOSS_BATCH = 0.322505, TIME 1491.016626\n",
      "STEP 700: LOSS_BATCH = 0.335379, TIME 1617.137093\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c = np.inf\n",
    "\n",
    "for i in range(50000):\n",
    "    batch = generate_batch(_dataset,_batch_size)\n",
    "    feed = {x: batch[0], x_gt:batch[1],drop :0.2}\n",
    "    summary,_ = sess.run([merged,optimizer], feed_dict=feed)\n",
    "    if i % _step_test == 0 and i>0:\n",
    "        \n",
    "        c_old = c\n",
    "        c = sess.run(cost, feed_dict=feed)\n",
    "        print(\"STEP %d: LOSS_BATCH = %f, TIME %f\"%(i,c,time.time()-start))\n",
    "        save_path = saver.save(sess, _train_folder+\"/model.ckpt\")\n",
    "        writer.add_summary(summary, i)\n",
    "    \n",
    "    if i % _step_viz == 0 and i>0:\n",
    "        test(i/_step_viz)\n",
    "    \n",
    "print(\"Optimization Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
