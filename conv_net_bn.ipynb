{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achitecture 1 : \"Learning Image Matching by Simply Watching Video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_conv_layers = [6,96,96,128,128,128]\n",
    "_activation = 'relu'\n",
    "\n",
    "_batch_size = 4 #16 in the paper but too expensive for the GPU\n",
    "_learning_rate = 1e-3\n",
    "_epochs = 20\n",
    "_step_test = 50\n",
    "_step_viz = 50\n",
    "\n",
    "_dataset = \"KITTI\"\n",
    "# _dataset = \"SINTEL\"\n",
    "\n",
    "if _dataset == \"KITTI\":\n",
    "    _h,_w = (128,384)\n",
    "if _dataset == \"SINTEL\":\n",
    "    _h,_w = (128,256)\n",
    "\n",
    "_data_folder = \"data/%s\"%_dataset\n",
    "_train_folder = \"train/%s\"%_dataset\n",
    "_previously_trained = False\n",
    "\n",
    "if not os.path.exists(_train_folder):\n",
    "    os.makedirs(_train_folder)\n",
    "    print(\"Directory created :\", _train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x,name=None):\n",
    "    if _activation == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x,name)\n",
    "    if _activation == 'relu':\n",
    "        return tf.nn.relu(x,name)\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(dataset,batch_size, train = 1, quality =\"clean\"):\n",
    "    \n",
    "    if dataset == \"SINTEL\":\n",
    "        frames = np.ndarray([3,batch_size,128,256, 3],np.float32)\n",
    "        if train:\n",
    "            folder = \"train\"\n",
    "        else:\n",
    "            folder = \"test\"\n",
    "        \n",
    "        seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality),batch_size)        \n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+2)+\".png\")\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]))\n",
    "            \n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n",
    "\n",
    "def load_batch(dataset,batch_size, train = 0):\n",
    "    ground_truth = cv2.imread(_train_folder+\"/ground_truth.png\")  \n",
    "    \n",
    "    ground_truth= np.split(ground_truth,batch_size,axis=0)\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for seq in os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]):\n",
    "                for index in range(len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-1):\n",
    "                    if (cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\") == ground_truth[i]).all():\n",
    "                    \n",
    "                        frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "                        frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "                        frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(\"float32\",[None,_h,_w,6],name='x_input')\n",
    "x_gt = tf.placeholder(\"float32\",[None,_h,_w,3],name='x_ground-truth')\n",
    "\n",
    "train = tf.constant(True)\n",
    "drop = tf.placeholder(\"float\")\n",
    "\n",
    "#CONVOLUTIONAL ENCODER\n",
    "\n",
    "with tf.variable_scope(\"CONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W1_1 = weight_variable([3,3,_conv_layers[0],_conv_layers[1]])\n",
    "        b1_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti1_1 = tf.nn.bias_add(tf.nn.conv2d(x,W1_1,[1,1,1,1],padding='SAME'),b1_1)        \n",
    "        bn1_1 = batch_norm(preacti1_1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv1_1 = activation(bn1_1,name=scope.name)\n",
    "        drop1_1 = tf.nn.dropout(conv1_1,1-drop)\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W2_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b2_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti2_1 = tf.nn.bias_add(tf.nn.conv2d(drop1_1,W2_1,[1,1,1,1],padding='SAME'),b2_1)\n",
    "        bn2_1 = batch_norm(preacti2_1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv2_1 = activation(bn2_1,name=scope.name)\n",
    "        drop2_1 = tf.nn.dropout(conv2_1,1-drop)\n",
    "        \n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        W3_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b3_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti3_1 = tf.nn.bias_add(tf.nn.conv2d(drop2_1,W3_1,[1,1,1,1],padding='SAME'),b3_1)\n",
    "        bn3_1 = batch_norm(preacti3_1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv3_1 = activation(bn3_1,name=scope.name)\n",
    "        drop3_1 = tf.nn.dropout(conv3_1,1-drop)\n",
    "        \n",
    "    pool_1 = tf.nn.max_pool(drop3_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(2,6):    \n",
    "    with tf.variable_scope(\"CONV-BLOCK%s\"%i) as scope:\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            globals()['W1_%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i]])\n",
    "            globals()['b1_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti1_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['pool_%s'%(i-1)],globals()['W1_%s'%i],[1,1,1,1],padding='SAME'),globals()['b1_%s'%i])\n",
    "            globals()['bn1_%s'%i] = batch_norm(globals()['preacti1_%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv1_%s'%i] = activation(globals()['bn1_%s'%i],name=scope.name)\n",
    "            globals()['drop1_%s'%i] = tf.nn.dropout(globals()['conv1_%s'%i],1-drop)\n",
    "            \n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            globals()['W2_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b2_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti2_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop1_%s'%i],globals()['W2_%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_%s'%i])\n",
    "            globals()['bn2_%s'%i] = batch_norm(globals()['preacti2_%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv2_%s'%i] = activation(globals()['bn2_%s'%i],name=scope.name)\n",
    "            globals()['drop2_%s'%i] = tf.nn.dropout(globals()['conv2_%s'%i],1-drop)\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            globals()['W3_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b3_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti3_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop2_%s'%i],globals()['W3_%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_%s'%i])\n",
    "            globals()['bn3_%s'%i] = batch_norm(globals()['preacti3_%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv3_%s'%i] = activation(globals()['bn3_%s'%i],name=scope.name)\n",
    "            globals()['drop3_%s'%i] = tf.nn.dropout(globals()['conv3_%s'%i],1-drop)\n",
    "\n",
    "        globals()['pool_%s'%i] = tf.nn.max_pool(globals()['drop3_%s'%i], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "\n",
    "#DECONVOLUTIONAL DECODER\n",
    "\n",
    "batch_size = tf.shape(x)[0]\n",
    "\n",
    "with tf.variable_scope(\"DECONV-BLOCK5\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d5 = weight_variable([4,4,_conv_layers[5],_conv_layers[4]])\n",
    "        b1_d5 = bias_variable([_conv_layers[4]])\n",
    "        convtr_d5 = tf.nn.conv2d_transpose(pool_5,W1_d5,[batch_size,int(_h/2**4),int(_w/2**4),_conv_layers[4]],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d5 = tf.nn.bias_add(convtr_d5,b1_d5)\n",
    "        bn1_d5 = batch_norm(preacti1_d5,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv1_d5 = activation(bn1_d5,name=scope.name)\n",
    "        drop1_d5 = tf.nn.dropout(conv1_d5,1-drop)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b2_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti2_d5 = tf.nn.bias_add(tf.nn.conv2d(drop1_d5,W2_d5,[1,1,1,1],padding='SAME'),b2_d5)\n",
    "        bn2_d5 = batch_norm(preacti2_d5,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv2_d5 = activation(bn2_d5,name=scope.name)\n",
    "        drop2_d5 = tf.nn.dropout(conv2_d5,1-drop)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b3_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti3_d5 = tf.nn.bias_add(tf.nn.conv2d(drop2_d5,W3_d5,[1,1,1,1],padding='SAME'),b3_d5)\n",
    "        bn3_d5 = batch_norm(preacti3_d5,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv3_d5 = activation(bn3_d5,name=scope.name)\n",
    "        drop3_d5 = tf.nn.dropout(conv3_d5,1-drop)\n",
    "\n",
    "        \n",
    "for i in range(4,1,-1):\n",
    "    with tf.variable_scope(\"DECONV-BLOCK%s\"%i) as scope:\n",
    "        \n",
    "        globals()['concat_d%s'%i] = tf.concat(3,[globals()['drop3_d%s'%(i+1)],globals()['pool_%s'%(i)]])\n",
    "        \n",
    "        with tf.variable_scope('deconv1') as scope:\n",
    "            globals()['W1_d%s'%i] = weight_variable([4,4,_conv_layers[i-1],2*_conv_layers[i]])\n",
    "            globals()['b1_d%s'%i] = bias_variable([_conv_layers[i-1]])\n",
    "            globals()['convtr_d%s'%i] = tf.nn.conv2d_transpose(globals()['concat_d%s'%i],globals()['W1_d%s'%i],[batch_size,int(_h/2**(i-1)),int(_w/2**(i-1)),_conv_layers[i-1]],[1,2,2,1],padding='SAME')\n",
    "            globals()['preacti1_d%s'%i] = tf.nn.bias_add(globals()['convtr_d%s'%i],globals()['b1_d%s'%i])\n",
    "            globals()['bn1_d%s'%i] = batch_norm(globals()['preacti1_d%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv1_d%s'%i] = activation(globals()['bn1_d%s'%i],name=scope.name)\n",
    "            globals()['drop1_d%s'%i] = tf.nn.dropout(globals()['conv1_d%s'%i],1-drop)\n",
    "            \n",
    "        with tf.variable_scope('deconv2') as scope:\n",
    "            globals()['W2_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b2_d%s'%i] = bias_variable([_conv_layers[i-1]])  \n",
    "            globals()['preacti2_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop1_d%s'%i],globals()['W2_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_d%s'%i])\n",
    "            globals()['bn2_d%s'%i] = batch_norm(globals()['preacti2_d%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv2_d%s'%i] = activation(globals()['bn2_d%s'%i],name=scope.name)\n",
    "            globals()['drop2_d%s'%i] = tf.nn.dropout(globals()['conv2_d%s'%i],1-drop)\n",
    "            \n",
    "        with tf.variable_scope('deconv3') as scope:\n",
    "            globals()['W3_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b3_d%s'%i] = bias_variable([_conv_layers[i-1]])             \n",
    "            globals()['preacti3_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['drop2_d%s'%i],globals()['W3_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_d%s'%i])\n",
    "            globals()['bn3_d%s'%i] = batch_norm(globals()['preacti3_d%s'%i],decay = 0.9, center = True, scale = True, is_training = train)\n",
    "            globals()['conv3_d%s'%i] = activation(globals()['bn3_d%s'%i],name=scope.name)\n",
    "            globals()['drop3_d%s'%i] = tf.nn.dropout(globals()['conv3_d%s'%i],1-drop)\n",
    "            \n",
    "with tf.variable_scope(\"DECONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d1 = weight_variable([4,4,3,_conv_layers[1]])\n",
    "        b1_d1 = bias_variable([3])\n",
    "        convtr_d1 = tf.nn.conv2d_transpose(conv3_d2,W1_d1,[batch_size,_h,_w,3],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d1 = tf.nn.bias_add(convtr_d1,b1_d1)\n",
    "        bn1_d1 = batch_norm(preacti1_d1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv1_d1 = activation(bn1_d1,name=scope.name)\n",
    "        drop1_d1 = tf.nn.dropout(conv1_d1,1-drop)\n",
    "        \n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d1 = weight_variable([3,3,3,3])\n",
    "        b2_d1 = bias_variable([3])             \n",
    "        preacti2_d1 = tf.nn.bias_add(tf.nn.conv2d(conv1_d1,W2_d1,[1,1,1,1],padding='SAME'),b2_d1)\n",
    "        bn2_d1 = batch_norm(preacti2_d1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv2_d1 = activation(bn2_d1,name=scope.name)\n",
    "        drop2_d1 = tf.nn.dropout(conv2_d1,1-drop)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d1 = weight_variable([3,3,3,3])\n",
    "        b3_d1 = bias_variable([3])             \n",
    "        preacti3_d1 = tf.nn.bias_add(tf.nn.conv2d(conv2_d1,W3_d1,[1,1,1,1],padding='SAME'),b3_d1)\n",
    "        bn3_d1 = batch_norm(preacti3_d1,decay = 0.9, center = True, scale = True, is_training = train)\n",
    "        conv3_d1 = activation(bn3_d1,name=scope.name)\n",
    "        drop3_d1 = tf.nn.dropout(conv3_d1,1-drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added batch norm and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Â Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charbonnier_loss(x):\n",
    "    return tf.sqrt(0.1**2+tf.reduce_mean(tf.square(x)))\n",
    "    \n",
    "def interpol_error(x,gt):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x,gt))))\n",
    "\n",
    "def normalized_interpol_error(x,gt):\n",
    "    norm_grad = cv2.Sobel(gt,cv2.CV_64F,1,0,ksize=5)**2+cv2.Sobel(gt,cv2.CV_64F,0,1,ksize=5)**2 \n",
    "    return tf.sqrt(tf.reduce_mean(tf.div(tf.square(tf.sub(x,gt)),(norm_grad+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    cost = charbonnier_loss(tf.sub(conv3_d1,x_gt))\n",
    "    tf.summary.scalar('loss',cost)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(_learning_rate, global_step,10000, 0.99, staircase=True)\n",
    "tf.summary.scalar('learning_rate',learning_rate)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(_train_folder+'/cnn', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#launch graph\n",
    "sess.run(init)\n",
    "\n",
    "# restore weights from model\n",
    "if _previously_trained :\n",
    "    saver.restore(sess, _train_folder+\"/model.ckpt\")\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# test_frames = generate_batch(_dataset,8,train = 0)\n",
    "# cv2.imwrite(_train_folder+\"/ground_truth.png\",np.concatenate(255*test_frames[1],0))\n",
    "\n",
    "test_frames = load_batch(_dataset,8,train=0)\n",
    "cv2.imwrite(_train_folder+\"/ground_truth2.png\",np.concatenate(255*test_frames[1],0))\n",
    "\n",
    "def test(i):\n",
    "    x_reconstruct =  sess.run(conv3_d1, feed_dict={x: test_frames[0], x_gt:test_frames[1], drop:0})   \n",
    "    \n",
    "#     cv2.namedWindow('reconstruction',flags= cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('reconstruction',np.concatenate(x_reconstruct,0))\n",
    "        \n",
    "#     cv2.namedWindow('ground-truth',flags= cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('ground-truth',np.concatenate(test_frames[1],0))\n",
    "    \n",
    "    cv2.imwrite(_train_folder+\"/reconstr_%s\"%'{0:04}'.format(int(i))+\".png\",255*np.concatenate(x_reconstruct,0))\n",
    "    cv2.imwrite(_train_folder+\"/reconstr.png\",255*np.concatenate(x_reconstruct,0))       \n",
    "#     cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "STEP 10: LOSS_BATCH = 0.345222, TIME 22.000283\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "STEP 20: LOSS_BATCH = 0.324830, TIME 59.622262\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "STEP 30: LOSS_BATCH = 0.317367, TIME 94.842027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b102d13b5d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STEP %d: LOSS_BATCH = %f, TIME %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_train_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1384\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1385\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m         clear_devices=clear_devices)\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1642\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m       \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[0;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopying\u001b[0m \u001b[0meach\u001b[0m \u001b[0mindividual\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mextend\u001b[0;34m(self, elem_seq)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mnew_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;31m# self._message_listener.Modified() not required here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;31m# mutations to submessages already propagate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/message.py\u001b[0m in \u001b[0;36mCopyFrom\u001b[0;34m(self, other_msg)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mother_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36m_Clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mModified\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener_for_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_UpdateOneofState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mModified\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c = np.inf\n",
    "\n",
    "for i in range(50000):\n",
    "    batch = generate_batch(_dataset,_batch_size)\n",
    "    feed = {x: batch[0], x_gt:batch[1],drop :0.2}\n",
    "    summary,_ = sess.run([merged,optimizer], feed_dict=feed)\n",
    "    if i % _step_test == 0 and i>0:\n",
    "        \n",
    "        c_old = c\n",
    "        c = sess.run(cost, feed_dict=feed)\n",
    "        print(\"STEP %d: LOSS_BATCH = %f, TIME %f\"%(i,c,time.time()-start))\n",
    "        save_path = saver.save(sess, _train_folder+\"/model.ckpt\")\n",
    "        writer.add_summary(summary, i)\n",
    "    \n",
    "    if i % _step_viz == 0 and i>0:\n",
    "        test(i/_step_viz)\n",
    "    \n",
    "print(\"Optimization Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
