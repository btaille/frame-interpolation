{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achitecture 1 : \"Learning Image Matching by Simply Watching Video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_conv_layers = [6,96,96,128,128,128]\n",
    "_activation = 'relu'\n",
    "\n",
    "_batch_size = 16\n",
    "_learning_rate = 1e-3\n",
    "_epochs = 20\n",
    "\n",
    "_dataset = \"KITTI\"\n",
    "\n",
    "if _dataset == \"KITTI\":\n",
    "    _h,_w = (128,384)\n",
    "if _dataset == \"SINTEL\":\n",
    "    _h,_w = (128,256)\n",
    "\n",
    "_data_folder = \"data/%s\"%_dataset\n",
    "_train_folder = \"train/%s\"%_dataset\n",
    "_previously_trained = True\n",
    "\n",
    "if not os.path.exists(_train_folder):\n",
    "    os.makedirs(_train_folder)\n",
    "    print(\"Directory created :\", _train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x,name=None):\n",
    "    if _activation == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x,name)\n",
    "    if _activation == 'relu':\n",
    "        return tf.nn.relu(x,name)\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(\"float\",[None,_h,_w,6],name='x_input')\n",
    "x_gt = tf.placeholder(\"float\",[None,_h,_w,3],name='x_truth')\n",
    "\n",
    "batch_size = tf.shape(x)[0]\n",
    "\n",
    "#CONVOLUTIONAL ENCODER\n",
    "\n",
    "with tf.variable_scope(\"CONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W1_1 = weight_variable([3,3,_conv_layers[0],_conv_layers[1]])\n",
    "        b1_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti1_1 = tf.nn.bias_add(tf.nn.conv2d(x,W1_1,[1,1,1,1],padding='SAME'),b1_1)\n",
    "        conv1_1 = activation(preacti1_1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W2_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b2_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti2_1 = tf.nn.bias_add(tf.nn.conv2d(conv1_1,W2_1,[1,1,1,1],padding='SAME'),b2_1)\n",
    "        conv2_1 = activation(preacti2_1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        W3_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b3_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti3_1 = tf.nn.bias_add(tf.nn.conv2d(conv2_1,W3_1,[1,1,1,1],padding='SAME'),b3_1)\n",
    "        conv3_1 = activation(preacti3_1,name=scope.name)\n",
    "        \n",
    "    pool_1 = tf.nn.max_pool(conv3_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(2,6):    \n",
    "    with tf.variable_scope(\"CONV-BLOCK%s\"%i) as scope:\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            globals()['W1_%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i]])\n",
    "            globals()['b1_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti1_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['pool_%s'%(i-1)],globals()['W1_%s'%i],[1,1,1,1],padding='SAME'),globals()['b1_%s'%i])\n",
    "            globals()['conv1_%s'%i] = activation(globals()['preacti1_%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            globals()['W2_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b2_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti2_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv1_%s'%i],globals()['W2_%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_%s'%i])\n",
    "            globals()['conv2_%s'%i] = activation(globals()['preacti2_%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            globals()['W3_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b3_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti3_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv2_%s'%i],globals()['W3_%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_%s'%i])\n",
    "            globals()['conv3_%s'%i] = activation(globals()['preacti3_%s'%i],name=scope.name)\n",
    "\n",
    "        globals()['pool_%s'%i] = tf.nn.max_pool(globals()['conv3_%s'%i], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    \n",
    "    \n",
    "#DECONVOLUTIONAL DECODER\n",
    "\n",
    "with tf.variable_scope(\"DECONV-BLOCK5\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d5 = weight_variable([4,4,_conv_layers[5],_conv_layers[4]])\n",
    "        b1_d5 = bias_variable([_conv_layers[4]])\n",
    "        convtr_d5 = tf.nn.conv2d_transpose(pool_5,W1_d5,[batch_size,int(_h/2**4),int(_w/2**4),_conv_layers[4]],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d5 = tf.nn.bias_add(convtr_d5,b1_d5)\n",
    "        conv1_d5 = activation(preacti1_d5,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b2_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti2_d5 = tf.nn.bias_add(tf.nn.conv2d(conv1_d5,W2_d5,[1,1,1,1],padding='SAME'),b2_d5)\n",
    "        conv2_d5 = activation(preacti2_d5,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b3_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti3_d5 = tf.nn.bias_add(tf.nn.conv2d(conv2_d5,W3_d5,[1,1,1,1],padding='SAME'),b3_d5)\n",
    "        conv3_d5 = activation(preacti3_d5,name=scope.name)\n",
    "\n",
    "        \n",
    "for i in range(4,1,-1):\n",
    "    with tf.variable_scope(\"DECONV-BLOCK%s\"%i) as scope:\n",
    "       \n",
    "        globals()['concat_d%s'%i] = tf.concat(3,[globals()['conv3_d%s'%(i+1)],globals()['conv3_%s'%(i)]])\n",
    "        \n",
    "        with tf.variable_scope('deconv1') as scope:\n",
    "            globals()['W1_d%s'%i] = weight_variable([4,4,_conv_layers[i-1],2*_conv_layers[i]])\n",
    "            globals()['b1_d%s'%i] = bias_variable([_conv_layers[i-1]])\n",
    "            globals()['convtr_d%s'%i] = tf.nn.conv2d_transpose(globals()['concat_d%s'%i],globals()['W1_d%s'%i],[batch_size,int(_h/2**(i-1)),int(_w/2**(i-1)),_conv_layers[i-1]],[1,2,2,1],padding='SAME')\n",
    "            globals()['preacti1_d%s'%i] = tf.nn.bias_add(globals()['convtr_d%s'%i],globals()['b1_d%s'%i])\n",
    "            globals()['conv1_d%s'%i] = activation(globals()['preacti1_d%s'%i],name=scope.name)\n",
    "            \n",
    "        with tf.variable_scope('deconv2') as scope:\n",
    "            globals()['W2_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b2_d%s'%i] = bias_variable([_conv_layers[i-1]])  \n",
    "            globals()['preacti2_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv1_d%s'%i],globals()['W2_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_d%s'%i])\n",
    "            globals()['conv2_d%s'%i] = activation(globals()['preacti2_d%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('deconv3') as scope:\n",
    "            globals()['W3_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b3_d%s'%i] = bias_variable([_conv_layers[i-1]])             \n",
    "            globals()['preacti3_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv2_d%s'%i],globals()['W3_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_d%s'%i])\n",
    "            globals()['conv3_d%s'%i] = activation(globals()['preacti3_d%s'%i],name=scope.name)\n",
    "            \n",
    "            \n",
    "with tf.variable_scope(\"DECONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d1 = weight_variable([4,4,3,_conv_layers[1]])\n",
    "        b1_d1 = bias_variable([3])\n",
    "        convtr_d1 = tf.nn.conv2d_transpose(conv3_d2,W1_d1,[batch_size,_h,_w,3],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d1 = tf.nn.bias_add(convtr_d1,b1_d1)\n",
    "        conv1_d1 = activation(preacti1_d1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d1 = weight_variable([3,3,3,3])\n",
    "        b2_d1 = bias_variable([3])             \n",
    "        preacti2_d1 = tf.nn.bias_add(tf.nn.conv2d(conv1_d1,W2_d1,[1,1,1,1],padding='SAME'),b2_d1)\n",
    "        conv2_d1 = activation(preacti2_d1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d1 = weight_variable([3,3,3,3])\n",
    "        b3_d1 = bias_variable([3])             \n",
    "        preacti3_d1 = tf.nn.bias_add(tf.nn.conv2d(conv2_d1,W3_d1,[1,1,1,1],padding='SAME'),b3_d1)\n",
    "        conv3_d1 = activation(preacti3_d1,name=scope.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential improvements : add dropout and batch normalization (not in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charbonnier_loss(x):\n",
    "    return tf.sqrt(0.1**2+tf.reduce_mean(tf.square(x)))\n",
    "    \n",
    "def interpol_error(x,gt):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x,gt))))\n",
    "\n",
    "def normalized_interpol_error(x,gt):\n",
    "    norm_grad = cv2.Sobel(gt,cv2.CV_64F,1,0,ksize=5)**2+cv2.Sobel(gt,cv2.CV_64F,0,1,ksize=5)**2 \n",
    "    return tf.sqrt(tf.reduce_mean(tf.div(tf.square(tf.sub(x,gt)),(norm_grad+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    cost = charbonnier_loss(tf.sub(conv3_d1,x_gt))\n",
    "    tf.summary.scalar('loss',cost)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(_learning_rate, global_step,10000, 0.99, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(_train_folder+'/cnn', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#launch graph\n",
    "sess.run(init)\n",
    "\n",
    "# restore weights from model\n",
    "if os.path.exists(_train_folder+\"/model.ckpt\"):\n",
    "    saver.restore(sess, _train_folder+\"/model.ckpt\")\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
