{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achitecture 1 : \"Learning Image Matching by Simply Watching Video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_conv_layers = [6,96,96,128,128,128]\n",
    "_activation = 'relu'\n",
    "\n",
    "_batch_size = 8 #16 in the paper but too expensive for the GPU\n",
    "_learning_rate = 1e-3\n",
    "_epochs = 20\n",
    "_step_test = 50\n",
    "_step_viz = 50\n",
    "\n",
    "# _dataset = \"KITTI\"\n",
    "_dataset = \"SINTEL\"\n",
    "\n",
    "if _dataset == \"KITTI\":\n",
    "    _h,_w = (128,384)\n",
    "if _dataset == \"SINTEL\":\n",
    "    _h,_w = (128,256)\n",
    "\n",
    "_data_folder = \"data/%s\"%_dataset\n",
    "_train_folder = \"train/%s\"%_dataset\n",
    "_previously_trained = True\n",
    "\n",
    "if not os.path.exists(_train_folder):\n",
    "    os.makedirs(_train_folder)\n",
    "    print(\"Directory created :\", _train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x,name=None):\n",
    "    if _activation == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x,name)\n",
    "    if _activation == 'relu':\n",
    "        return tf.nn.relu(x,name)\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(dataset,batch_size, train = 1, quality =\"clean\"):\n",
    "    \n",
    "    if dataset == \"SINTEL\":\n",
    "        frames = np.ndarray([3,batch_size,128,256, 3],np.float32)\n",
    "        if train:\n",
    "            folder = \"train\"\n",
    "        else:\n",
    "            folder = \"test\"\n",
    "        \n",
    "        seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality),batch_size)        \n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/\"+folder+\"/\"+quality+\"/\"+seq[i]+\"/frame_%s\"%'{0:04}'.format(index+2)+\".png\")\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            seq = np.random.choice(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]))\n",
    "            \n",
    "            index = np.random.randint(1,len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-2)            \n",
    "            frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "            frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "            frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n",
    "\n",
    "def load_batch(dataset,batch_size, train = 0):\n",
    "    ground_truth = cv2.imread(_train_folder+\"/ground_truth.png\")  \n",
    "    \n",
    "    ground_truth= np.split(ground_truth,batch_size,axis=0)\n",
    "    \n",
    "    if dataset == \"KITTI\":\n",
    "        frames = np.ndarray([3,batch_size,128,384, 3],np.float32)\n",
    "        if train:\n",
    "            folders = [\"2011_09_26\",\"2011_09_28\",\"2011_09_30\",\"2011_10_03\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        else:\n",
    "            folders = [\"2011_09_29\"]\n",
    "            folder = np.random.choice(folders,batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for seq in os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]):\n",
    "                for index in range(len(os.listdir(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq))-1):\n",
    "                    if (cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\") == ground_truth[i]).all():\n",
    "                    \n",
    "                        frames[0,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index)+\".png\")\n",
    "                        frames[1,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+1)+\".png\")\n",
    "                        frames[2,i,:,:,:] = cv2.imread(\"data/\"+dataset+\"/kitti_resized/\"+folder[i]+\"/\"+seq+\"/%s\"%'{0:010}'.format(index+2)+\".png\")\n",
    "    \n",
    "    return np.concatenate([frames[0],frames[2]],axis=3)/255.,frames[1]/255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(\"float32\",[None,_h,_w,6],name='x_input')\n",
    "x_gt = tf.placeholder(\"float32\",[None,_h,_w,3],name='x_ground-truth')\n",
    "\n",
    "#CONVOLUTIONAL ENCODER\n",
    "\n",
    "with tf.variable_scope(\"CONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W1_1 = weight_variable([3,3,_conv_layers[0],_conv_layers[1]])\n",
    "        b1_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti1_1 = tf.nn.bias_add(tf.nn.conv2d(x,W1_1,[1,1,1,1],padding='SAME'),b1_1)\n",
    "        conv1_1 = activation(preacti1_1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        W2_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b2_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti2_1 = tf.nn.bias_add(tf.nn.conv2d(conv1_1,W2_1,[1,1,1,1],padding='SAME'),b2_1)\n",
    "        conv2_1 = activation(preacti2_1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        W3_1 = weight_variable([3,3,_conv_layers[1],_conv_layers[1]])\n",
    "        b3_1 = bias_variable([_conv_layers[1]])             \n",
    "        preacti3_1 = tf.nn.bias_add(tf.nn.conv2d(conv2_1,W3_1,[1,1,1,1],padding='SAME'),b3_1)\n",
    "        conv3_1 = activation(preacti3_1,name=scope.name)\n",
    "        \n",
    "    pool_1 = tf.nn.max_pool(conv3_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(2,6):    \n",
    "    with tf.variable_scope(\"CONV-BLOCK%s\"%i) as scope:\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            globals()['W1_%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i]])\n",
    "            globals()['b1_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti1_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['pool_%s'%(i-1)],globals()['W1_%s'%i],[1,1,1,1],padding='SAME'),globals()['b1_%s'%i])\n",
    "            globals()['conv1_%s'%i] = activation(globals()['preacti1_%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            globals()['W2_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b2_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti2_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv1_%s'%i],globals()['W2_%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_%s'%i])\n",
    "            globals()['conv2_%s'%i] = activation(globals()['preacti2_%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            globals()['W3_%s'%i] = weight_variable([3,3,_conv_layers[i],_conv_layers[i]])\n",
    "            globals()['b3_%s'%i] = bias_variable([_conv_layers[i]])             \n",
    "            globals()['preacti3_%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv2_%s'%i],globals()['W3_%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_%s'%i])\n",
    "            globals()['conv3_%s'%i] = activation(globals()['preacti3_%s'%i],name=scope.name)\n",
    "\n",
    "        globals()['pool_%s'%i] = tf.nn.max_pool(globals()['conv3_%s'%i], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "\n",
    "#DECONVOLUTIONAL DECODER\n",
    "\n",
    "batch_size = tf.shape(x)[0]\n",
    "\n",
    "with tf.variable_scope(\"DECONV-BLOCK5\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d5 = weight_variable([4,4,_conv_layers[5],_conv_layers[4]])\n",
    "        b1_d5 = bias_variable([_conv_layers[4]])\n",
    "        convtr_d5 = tf.nn.conv2d_transpose(pool_5,W1_d5,[batch_size,int(_h/2**4),int(_w/2**4),_conv_layers[4]],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d5 = tf.nn.bias_add(convtr_d5,b1_d5)\n",
    "        conv1_d5 = activation(preacti1_d5,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b2_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti2_d5 = tf.nn.bias_add(tf.nn.conv2d(conv1_d5,W2_d5,[1,1,1,1],padding='SAME'),b2_d5)\n",
    "        conv2_d5 = activation(preacti2_d5,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d5 = weight_variable([3,3,_conv_layers[4],_conv_layers[4]])\n",
    "        b3_d5 = bias_variable([_conv_layers[4]])             \n",
    "        preacti3_d5 = tf.nn.bias_add(tf.nn.conv2d(conv2_d5,W3_d5,[1,1,1,1],padding='SAME'),b3_d5)\n",
    "        conv3_d5 = activation(preacti3_d5,name=scope.name)\n",
    "\n",
    "        \n",
    "for i in range(4,1,-1):\n",
    "    with tf.variable_scope(\"DECONV-BLOCK%s\"%i) as scope:\n",
    "        \n",
    "        globals()['concat_d%s'%i] = tf.concat(3,[globals()['conv3_d%s'%(i+1)],globals()['pool_%s'%(i)]])\n",
    "        \n",
    "        with tf.variable_scope('deconv1') as scope:\n",
    "            globals()['W1_d%s'%i] = weight_variable([4,4,_conv_layers[i-1],2*_conv_layers[i]])\n",
    "            globals()['b1_d%s'%i] = bias_variable([_conv_layers[i-1]])\n",
    "            globals()['convtr_d%s'%i] = tf.nn.conv2d_transpose(globals()['concat_d%s'%i],globals()['W1_d%s'%i],[batch_size,int(_h/2**(i-1)),int(_w/2**(i-1)),_conv_layers[i-1]],[1,2,2,1],padding='SAME')\n",
    "            globals()['preacti1_d%s'%i] = tf.nn.bias_add(globals()['convtr_d%s'%i],globals()['b1_d%s'%i])\n",
    "            globals()['conv1_d%s'%i] = activation(globals()['preacti1_d%s'%i],name=scope.name)\n",
    "            \n",
    "        with tf.variable_scope('deconv2') as scope:\n",
    "            globals()['W2_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b2_d%s'%i] = bias_variable([_conv_layers[i-1]])  \n",
    "            globals()['preacti2_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv1_d%s'%i],globals()['W2_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b2_d%s'%i])\n",
    "            globals()['conv2_d%s'%i] = activation(globals()['preacti2_d%s'%i],name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('deconv3') as scope:\n",
    "            globals()['W3_d%s'%i] = weight_variable([3,3,_conv_layers[i-1],_conv_layers[i-1]])\n",
    "            globals()['b3_d%s'%i] = bias_variable([_conv_layers[i-1]])             \n",
    "            globals()['preacti3_d%s'%i] = tf.nn.bias_add(tf.nn.conv2d(globals()['conv2_d%s'%i],globals()['W3_d%s'%i],[1,1,1,1],padding='SAME'),globals()['b3_d%s'%i])\n",
    "            globals()['conv3_d%s'%i] = activation(globals()['preacti3_d%s'%i],name=scope.name)\n",
    "            \n",
    "            \n",
    "with tf.variable_scope(\"DECONV-BLOCK1\") as scope:\n",
    "    with tf.variable_scope('deconv1') as scope:\n",
    "        W1_d1 = weight_variable([4,4,3,_conv_layers[1]])\n",
    "        b1_d1 = bias_variable([3])\n",
    "        convtr_d1 = tf.nn.conv2d_transpose(conv3_d2,W1_d1,[batch_size,_h,_w,3],[1,2,2,1],padding='SAME')\n",
    "        preacti1_d1 = tf.nn.bias_add(convtr_d1,b1_d1)\n",
    "        conv1_d1 = activation(preacti1_d1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv2') as scope:\n",
    "        W2_d1 = weight_variable([3,3,3,3])\n",
    "        b2_d1 = bias_variable([3])             \n",
    "        preacti2_d1 = tf.nn.bias_add(tf.nn.conv2d(conv1_d1,W2_d1,[1,1,1,1],padding='SAME'),b2_d1)\n",
    "        conv2_d1 = activation(preacti2_d1,name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('deconv3') as scope:\n",
    "        W3_d1 = weight_variable([3,3,3,3])\n",
    "        b3_d1 = bias_variable([3])             \n",
    "        preacti3_d1 = tf.nn.bias_add(tf.nn.conv2d(conv2_d1,W3_d1,[1,1,1,1],padding='SAME'),b3_d1)\n",
    "        conv3_d1 = activation(preacti3_d1,name=scope.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential improvements : add dropout and batch normalization (not in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charbonnier_loss(x):\n",
    "    return tf.sqrt(0.1**2+tf.reduce_mean(tf.square(x)))\n",
    "    \n",
    "def interpol_error(x,gt):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x,gt))))\n",
    "\n",
    "def normalized_interpol_error(x,gt):\n",
    "    norm_grad = cv2.Sobel(gt,cv2.CV_64F,1,0,ksize=5)**2+cv2.Sobel(gt,cv2.CV_64F,0,1,ksize=5)**2 \n",
    "    return tf.sqrt(tf.reduce_mean(tf.div(tf.square(tf.sub(x,gt)),(norm_grad+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    cost = charbonnier_loss(tf.sub(conv3_d1,x_gt))\n",
    "    tf.summary.scalar('loss',cost)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(_learning_rate, global_step,10000, 0.99, staircase=True)\n",
    "tf.summary.scalar('learning_rate',learning_rate)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(_train_folder+'/cnn', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#launch graph\n",
    "sess.run(init)\n",
    "\n",
    "# restore weights from model\n",
    "saver.restore(sess, _train_folder+\"/model.ckpt\")\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "test_frames = generate_batch(_dataset,8,train = 0)\n",
    "cv2.imwrite(_train_folder+\"/ground_truth.png\",np.concatenate(255*test_frames[1],0))\n",
    "\n",
    "# test_frames = load_batch(_dataset,8,train=0)\n",
    "# cv2.imwrite(_train_folder+\"/ground_truth2.png\",np.concatenate(255*test_frames[1],0))\n",
    "\n",
    "def test(i):\n",
    "    x_reconstruct =  sess.run(conv3_d1, feed_dict={x: test_frames[0], x_gt:test_frames[1]})   \n",
    "    \n",
    "#     cv2.namedWindow('reconstruction',flags= cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('reconstruction',np.concatenate(x_reconstruct,0))\n",
    "        \n",
    "#     cv2.namedWindow('ground-truth',flags= cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('ground-truth',np.concatenate(test_frames[1],0))\n",
    "    \n",
    "    cv2.imwrite(_train_folder+\"/reconstr_%s\"%'{0:04}'.format(int(i))+\".png\",255*np.concatenate(x_reconstruct,0))\n",
    "    cv2.imwrite(_train_folder+\"/reconstr.png\",255*np.concatenate(x_reconstruct,0))       \n",
    "#     cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 50: LOSS_BATCH = 0.127344, TIME 63.776841\n",
      "STEP 100: LOSS_BATCH = 0.143724, TIME 125.489867\n",
      "STEP 150: LOSS_BATCH = 0.143138, TIME 184.751572\n",
      "STEP 200: LOSS_BATCH = 0.118259, TIME 244.764648\n",
      "STEP 250: LOSS_BATCH = 0.143966, TIME 310.909061\n",
      "STEP 300: LOSS_BATCH = 0.122550, TIME 372.536635\n",
      "STEP 350: LOSS_BATCH = 0.125779, TIME 433.741466\n",
      "STEP 400: LOSS_BATCH = 0.117776, TIME 495.303548\n",
      "STEP 450: LOSS_BATCH = 0.129673, TIME 555.385583\n",
      "STEP 500: LOSS_BATCH = 0.141616, TIME 614.683901\n",
      "STEP 550: LOSS_BATCH = 0.118895, TIME 674.513459\n",
      "STEP 600: LOSS_BATCH = 0.119656, TIME 735.019205\n",
      "STEP 650: LOSS_BATCH = 0.126612, TIME 794.777233\n",
      "STEP 700: LOSS_BATCH = 0.128628, TIME 853.152426\n",
      "STEP 750: LOSS_BATCH = 0.141786, TIME 913.145631\n",
      "STEP 800: LOSS_BATCH = 0.136098, TIME 972.203053\n",
      "STEP 850: LOSS_BATCH = 0.133090, TIME 1030.082714\n",
      "STEP 900: LOSS_BATCH = 0.119431, TIME 1092.071673\n",
      "STEP 950: LOSS_BATCH = 0.121911, TIME 1151.504433\n",
      "STEP 1000: LOSS_BATCH = 0.136568, TIME 1212.487896\n",
      "STEP 1050: LOSS_BATCH = 0.133896, TIME 1272.189965\n",
      "STEP 1100: LOSS_BATCH = 0.123884, TIME 1331.705323\n",
      "STEP 1150: LOSS_BATCH = 0.121565, TIME 1391.425184\n",
      "STEP 1200: LOSS_BATCH = 0.123875, TIME 1450.038517\n",
      "STEP 1250: LOSS_BATCH = 0.121549, TIME 1509.502867\n",
      "STEP 1300: LOSS_BATCH = 0.130138, TIME 1568.064002\n",
      "STEP 1350: LOSS_BATCH = 0.129290, TIME 1626.373493\n",
      "STEP 1400: LOSS_BATCH = 0.117018, TIME 1688.738104\n",
      "STEP 1450: LOSS_BATCH = 0.124330, TIME 1748.311140\n",
      "STEP 1500: LOSS_BATCH = 0.136273, TIME 1808.248719\n",
      "STEP 1550: LOSS_BATCH = 0.109890, TIME 1868.398306\n",
      "STEP 1600: LOSS_BATCH = 0.121378, TIME 1928.944483\n",
      "STEP 1650: LOSS_BATCH = 0.120392, TIME 1989.258646\n",
      "STEP 1700: LOSS_BATCH = 0.119060, TIME 2050.456064\n",
      "STEP 1750: LOSS_BATCH = 0.120984, TIME 2112.314848\n",
      "STEP 1800: LOSS_BATCH = 0.122114, TIME 2172.551505\n",
      "STEP 1850: LOSS_BATCH = 0.117883, TIME 2233.272117\n",
      "STEP 1900: LOSS_BATCH = 0.131592, TIME 2291.886244\n",
      "STEP 1950: LOSS_BATCH = 0.118699, TIME 2351.865876\n",
      "STEP 2000: LOSS_BATCH = 0.121739, TIME 2412.734030\n",
      "STEP 2050: LOSS_BATCH = 0.120213, TIME 2474.190218\n",
      "STEP 2100: LOSS_BATCH = 0.127245, TIME 2533.361165\n",
      "STEP 2150: LOSS_BATCH = 0.124455, TIME 2593.504752\n",
      "STEP 2200: LOSS_BATCH = 0.117420, TIME 2654.694547\n",
      "STEP 2250: LOSS_BATCH = 0.120015, TIME 2714.500863\n",
      "STEP 2300: LOSS_BATCH = 0.117251, TIME 2773.477934\n",
      "STEP 2350: LOSS_BATCH = 0.135762, TIME 2834.064087\n",
      "STEP 2400: LOSS_BATCH = 0.118163, TIME 2894.208246\n",
      "STEP 2450: LOSS_BATCH = 0.121350, TIME 2954.428795\n",
      "STEP 2500: LOSS_BATCH = 0.128962, TIME 3014.131180\n",
      "STEP 2550: LOSS_BATCH = 0.111096, TIME 3074.614151\n",
      "STEP 2600: LOSS_BATCH = 0.118844, TIME 3134.542102\n",
      "STEP 2650: LOSS_BATCH = 0.120312, TIME 3194.032839\n",
      "STEP 2700: LOSS_BATCH = 0.119719, TIME 3255.115211\n",
      "STEP 2750: LOSS_BATCH = 0.117267, TIME 3318.227859\n",
      "STEP 2800: LOSS_BATCH = 0.121366, TIME 3378.025566\n",
      "STEP 2850: LOSS_BATCH = 0.120827, TIME 3435.998585\n",
      "STEP 2900: LOSS_BATCH = 0.122734, TIME 3496.216465\n",
      "STEP 2950: LOSS_BATCH = 0.123404, TIME 3557.103122\n",
      "STEP 3000: LOSS_BATCH = 0.122202, TIME 3615.999458\n",
      "STEP 3050: LOSS_BATCH = 0.113504, TIME 3676.370166\n",
      "STEP 3100: LOSS_BATCH = 0.116438, TIME 3734.787670\n",
      "STEP 3150: LOSS_BATCH = 0.118446, TIME 3795.558533\n",
      "STEP 3200: LOSS_BATCH = 0.124158, TIME 3856.783038\n",
      "STEP 3250: LOSS_BATCH = 0.116289, TIME 3917.466965\n",
      "STEP 3300: LOSS_BATCH = 0.117146, TIME 3977.849229\n",
      "STEP 3350: LOSS_BATCH = 0.112672, TIME 4040.260190\n",
      "STEP 3400: LOSS_BATCH = 0.118636, TIME 4098.619394\n",
      "STEP 3450: LOSS_BATCH = 0.118426, TIME 4157.388532\n",
      "STEP 3500: LOSS_BATCH = 0.118405, TIME 4219.625429\n",
      "STEP 3550: LOSS_BATCH = 0.123863, TIME 4279.249233\n",
      "STEP 3600: LOSS_BATCH = 0.123767, TIME 4338.737104\n",
      "STEP 3650: LOSS_BATCH = 0.120661, TIME 4398.542488\n",
      "STEP 3700: LOSS_BATCH = 0.114915, TIME 4459.777478\n",
      "STEP 3750: LOSS_BATCH = 0.120020, TIME 4519.888301\n",
      "STEP 3800: LOSS_BATCH = 0.114682, TIME 4578.128498\n",
      "STEP 3850: LOSS_BATCH = 0.125675, TIME 4636.083279\n",
      "STEP 3900: LOSS_BATCH = 0.120024, TIME 4696.249279\n",
      "STEP 3950: LOSS_BATCH = 0.116080, TIME 4757.292568\n",
      "STEP 4000: LOSS_BATCH = 0.112851, TIME 4815.872840\n",
      "STEP 4050: LOSS_BATCH = 0.124153, TIME 4879.773433\n",
      "STEP 4100: LOSS_BATCH = 0.114393, TIME 4939.292638\n",
      "STEP 4150: LOSS_BATCH = 0.115246, TIME 5000.830241\n",
      "STEP 4200: LOSS_BATCH = 0.117532, TIME 5060.795990\n",
      "STEP 4250: LOSS_BATCH = 0.114208, TIME 5119.771723\n",
      "STEP 4300: LOSS_BATCH = 0.123838, TIME 5179.804554\n",
      "STEP 4350: LOSS_BATCH = 0.114101, TIME 5240.021351\n",
      "STEP 4400: LOSS_BATCH = 0.119744, TIME 5299.251184\n",
      "STEP 4450: LOSS_BATCH = 0.114470, TIME 5358.453266\n",
      "STEP 4500: LOSS_BATCH = 0.116007, TIME 5419.327822\n",
      "STEP 4550: LOSS_BATCH = 0.117117, TIME 5479.303647\n",
      "STEP 4600: LOSS_BATCH = 0.115332, TIME 5537.508907\n",
      "STEP 4650: LOSS_BATCH = 0.116230, TIME 5597.601112\n",
      "STEP 4700: LOSS_BATCH = 0.117927, TIME 5657.506157\n",
      "STEP 4750: LOSS_BATCH = 0.116769, TIME 5716.599519\n",
      "STEP 4800: LOSS_BATCH = 0.115944, TIME 5777.537981\n",
      "STEP 4850: LOSS_BATCH = 0.110087, TIME 5836.669134\n",
      "STEP 4900: LOSS_BATCH = 0.120000, TIME 5897.543205\n",
      "STEP 4950: LOSS_BATCH = 0.116439, TIME 5958.127638\n",
      "STEP 5000: LOSS_BATCH = 0.113749, TIME 6016.494010\n",
      "STEP 5050: LOSS_BATCH = 0.117698, TIME 6075.458076\n",
      "STEP 5100: LOSS_BATCH = 0.112220, TIME 6135.734319\n",
      "STEP 5150: LOSS_BATCH = 0.113935, TIME 6195.224591\n",
      "STEP 5200: LOSS_BATCH = 0.123811, TIME 6253.994324\n",
      "STEP 5250: LOSS_BATCH = 0.113505, TIME 6314.239964\n",
      "STEP 5300: LOSS_BATCH = 0.122532, TIME 6373.878910\n",
      "STEP 5350: LOSS_BATCH = 0.114158, TIME 6434.091126\n",
      "STEP 5400: LOSS_BATCH = 0.115868, TIME 6493.557703\n",
      "STEP 5450: LOSS_BATCH = 0.117203, TIME 6552.809373\n",
      "STEP 5500: LOSS_BATCH = 0.122795, TIME 6611.291614\n",
      "STEP 5550: LOSS_BATCH = 0.128078, TIME 6670.488830\n",
      "STEP 5600: LOSS_BATCH = 0.117127, TIME 6731.375571\n",
      "STEP 5650: LOSS_BATCH = 0.118566, TIME 6792.400532\n",
      "STEP 5700: LOSS_BATCH = 0.115361, TIME 6851.654938\n",
      "STEP 5750: LOSS_BATCH = 0.128551, TIME 6910.053269\n",
      "STEP 5800: LOSS_BATCH = 0.115833, TIME 6971.545118\n",
      "STEP 5850: LOSS_BATCH = 0.114950, TIME 7031.159760\n",
      "STEP 5900: LOSS_BATCH = 0.117507, TIME 7091.135021\n",
      "STEP 5950: LOSS_BATCH = 0.117442, TIME 7151.858319\n",
      "STEP 6000: LOSS_BATCH = 0.117498, TIME 7210.195835\n",
      "STEP 6050: LOSS_BATCH = 0.115131, TIME 7269.986469\n",
      "STEP 6100: LOSS_BATCH = 0.112814, TIME 7330.346097\n",
      "STEP 6150: LOSS_BATCH = 0.112872, TIME 7388.046873\n",
      "STEP 6200: LOSS_BATCH = 0.115445, TIME 7449.010009\n",
      "STEP 6250: LOSS_BATCH = 0.113827, TIME 7508.774830\n",
      "STEP 6300: LOSS_BATCH = 0.111735, TIME 7569.267278\n",
      "STEP 6350: LOSS_BATCH = 0.121549, TIME 7630.681025\n",
      "STEP 6400: LOSS_BATCH = 0.114280, TIME 7691.263856\n",
      "STEP 6450: LOSS_BATCH = 0.108658, TIME 7751.063905\n",
      "STEP 6500: LOSS_BATCH = 0.118388, TIME 7810.292682\n",
      "STEP 6550: LOSS_BATCH = 0.113078, TIME 7868.113363\n",
      "STEP 6600: LOSS_BATCH = 0.115925, TIME 7929.151875\n",
      "STEP 6650: LOSS_BATCH = 0.112606, TIME 7990.212062\n",
      "STEP 6700: LOSS_BATCH = 0.115891, TIME 8050.316079\n",
      "STEP 6750: LOSS_BATCH = 0.116191, TIME 8109.954598\n",
      "STEP 6800: LOSS_BATCH = 0.111972, TIME 8168.364687\n",
      "STEP 6850: LOSS_BATCH = 0.111110, TIME 8228.327140\n",
      "STEP 6900: LOSS_BATCH = 0.117161, TIME 8289.148936\n",
      "STEP 6950: LOSS_BATCH = 0.116513, TIME 8347.874298\n",
      "STEP 7000: LOSS_BATCH = 0.116543, TIME 8407.211011\n",
      "STEP 7050: LOSS_BATCH = 0.115683, TIME 8467.190351\n",
      "STEP 7100: LOSS_BATCH = 0.114178, TIME 8526.915099\n",
      "STEP 7150: LOSS_BATCH = 0.110724, TIME 8585.997090\n",
      "STEP 7200: LOSS_BATCH = 0.118592, TIME 8649.056375\n",
      "STEP 7250: LOSS_BATCH = 0.117531, TIME 8710.036117\n",
      "STEP 7300: LOSS_BATCH = 0.109420, TIME 8771.415193\n",
      "STEP 7350: LOSS_BATCH = 0.118848, TIME 8834.775096\n",
      "STEP 7400: LOSS_BATCH = 0.117331, TIME 8895.871449\n",
      "STEP 7450: LOSS_BATCH = 0.116859, TIME 8956.663886\n",
      "STEP 7500: LOSS_BATCH = 0.117721, TIME 9016.657469\n",
      "STEP 7550: LOSS_BATCH = 0.110124, TIME 9076.034500\n",
      "STEP 7600: LOSS_BATCH = 0.119952, TIME 9134.848623\n",
      "STEP 7650: LOSS_BATCH = 0.111487, TIME 9193.459564\n",
      "STEP 7700: LOSS_BATCH = 0.114497, TIME 9252.738114\n",
      "STEP 7750: LOSS_BATCH = 0.118010, TIME 9311.785835\n",
      "STEP 7800: LOSS_BATCH = 0.119306, TIME 9372.460466\n",
      "STEP 7850: LOSS_BATCH = 0.114297, TIME 9433.859137\n",
      "STEP 7900: LOSS_BATCH = 0.119506, TIME 9497.182288\n",
      "STEP 7950: LOSS_BATCH = 0.119443, TIME 9555.196090\n",
      "STEP 8000: LOSS_BATCH = 0.117594, TIME 9618.636275\n",
      "STEP 8050: LOSS_BATCH = 0.117682, TIME 9678.420490\n",
      "STEP 8100: LOSS_BATCH = 0.115559, TIME 9738.974856\n",
      "STEP 8150: LOSS_BATCH = 0.115702, TIME 9798.298589\n",
      "STEP 8200: LOSS_BATCH = 0.108227, TIME 9857.971258\n",
      "STEP 8250: LOSS_BATCH = 0.115411, TIME 9917.862895\n",
      "STEP 8300: LOSS_BATCH = 0.112960, TIME 9978.839761\n",
      "STEP 8350: LOSS_BATCH = 0.117802, TIME 10039.280153\n",
      "STEP 8400: LOSS_BATCH = 0.114409, TIME 10101.177965\n",
      "STEP 8450: LOSS_BATCH = 0.110423, TIME 10160.620481\n",
      "STEP 8500: LOSS_BATCH = 0.119985, TIME 10219.962671\n",
      "STEP 8550: LOSS_BATCH = 0.121800, TIME 10280.033015\n",
      "STEP 8600: LOSS_BATCH = 0.117495, TIME 10339.680941\n",
      "STEP 8650: LOSS_BATCH = 0.120773, TIME 10400.984887\n",
      "STEP 8700: LOSS_BATCH = 0.114799, TIME 10460.220591\n",
      "STEP 8750: LOSS_BATCH = 0.113596, TIME 10520.932204\n",
      "STEP 8800: LOSS_BATCH = 0.118562, TIME 10581.121275\n",
      "STEP 8850: LOSS_BATCH = 0.121630, TIME 10641.746241\n",
      "STEP 8900: LOSS_BATCH = 0.121660, TIME 10700.435197\n",
      "STEP 8950: LOSS_BATCH = 0.115043, TIME 10759.595006\n",
      "STEP 9000: LOSS_BATCH = 0.123046, TIME 10821.214796\n",
      "STEP 9050: LOSS_BATCH = 0.111876, TIME 10882.912543\n",
      "STEP 9100: LOSS_BATCH = 0.120123, TIME 10942.674435\n",
      "STEP 9150: LOSS_BATCH = 0.117549, TIME 11002.720233\n",
      "STEP 9200: LOSS_BATCH = 0.114480, TIME 11065.366687\n",
      "STEP 9250: LOSS_BATCH = 0.115885, TIME 11125.147478\n",
      "STEP 9300: LOSS_BATCH = 0.113199, TIME 11185.851006\n",
      "STEP 9350: LOSS_BATCH = 0.113944, TIME 11245.120204\n",
      "STEP 9400: LOSS_BATCH = 0.115849, TIME 11304.176594\n",
      "STEP 9450: LOSS_BATCH = 0.115312, TIME 11365.757297\n",
      "STEP 9500: LOSS_BATCH = 0.115534, TIME 11426.912046\n",
      "STEP 9550: LOSS_BATCH = 0.115339, TIME 11487.043204\n",
      "STEP 9600: LOSS_BATCH = 0.108109, TIME 11544.929767\n",
      "STEP 9650: LOSS_BATCH = 0.113902, TIME 11603.956887\n",
      "STEP 9700: LOSS_BATCH = 0.116423, TIME 11664.593217\n",
      "STEP 9750: LOSS_BATCH = 0.110898, TIME 11723.915933\n",
      "STEP 9800: LOSS_BATCH = 0.115894, TIME 11784.803325\n",
      "STEP 9850: LOSS_BATCH = 0.110239, TIME 11844.390522\n",
      "STEP 9900: LOSS_BATCH = 0.110679, TIME 11904.696545\n",
      "STEP 9950: LOSS_BATCH = 0.111931, TIME 11964.758264\n",
      "STEP 10000: LOSS_BATCH = 0.112534, TIME 12023.668608\n",
      "STEP 10050: LOSS_BATCH = 0.116367, TIME 12084.463848\n",
      "STEP 10100: LOSS_BATCH = 0.108385, TIME 12143.067634\n",
      "STEP 10150: LOSS_BATCH = 0.112657, TIME 12202.557784\n",
      "STEP 10200: LOSS_BATCH = 0.113528, TIME 12260.619389\n",
      "STEP 10250: LOSS_BATCH = 0.114852, TIME 12320.194561\n",
      "STEP 10300: LOSS_BATCH = 0.111939, TIME 12379.683069\n",
      "STEP 10350: LOSS_BATCH = 0.117355, TIME 12439.649170\n",
      "STEP 10400: LOSS_BATCH = 0.118441, TIME 12498.545903\n",
      "STEP 10450: LOSS_BATCH = 0.114107, TIME 12561.331976\n",
      "STEP 10500: LOSS_BATCH = 0.112282, TIME 12621.348403\n",
      "STEP 10550: LOSS_BATCH = 0.112031, TIME 12680.535784\n",
      "STEP 10600: LOSS_BATCH = 0.112062, TIME 12740.444499\n",
      "STEP 10650: LOSS_BATCH = 0.109328, TIME 12800.321455\n",
      "STEP 10700: LOSS_BATCH = 0.111790, TIME 12861.302090\n",
      "STEP 10750: LOSS_BATCH = 0.109232, TIME 12921.173250\n",
      "STEP 10800: LOSS_BATCH = 0.113184, TIME 12981.677120\n",
      "STEP 10850: LOSS_BATCH = 0.117986, TIME 13041.877652\n",
      "STEP 10900: LOSS_BATCH = 0.109124, TIME 13103.512294\n",
      "STEP 10950: LOSS_BATCH = 0.116248, TIME 13163.018040\n",
      "STEP 11000: LOSS_BATCH = 0.111264, TIME 13222.384742\n",
      "STEP 11050: LOSS_BATCH = 0.114898, TIME 13280.564230\n",
      "STEP 11100: LOSS_BATCH = 0.114442, TIME 13338.955189\n",
      "STEP 11150: LOSS_BATCH = 0.111798, TIME 13399.905069\n",
      "STEP 11200: LOSS_BATCH = 0.116345, TIME 13461.172043\n",
      "STEP 11250: LOSS_BATCH = 0.113155, TIME 13519.627591\n",
      "STEP 11300: LOSS_BATCH = 0.112151, TIME 13580.579836\n",
      "STEP 11350: LOSS_BATCH = 0.111272, TIME 13640.179886\n",
      "STEP 11400: LOSS_BATCH = 0.117781, TIME 13699.626966\n",
      "STEP 11450: LOSS_BATCH = 0.111432, TIME 13757.472991\n",
      "STEP 11500: LOSS_BATCH = 0.114384, TIME 13818.901388\n",
      "STEP 11550: LOSS_BATCH = 0.113538, TIME 13878.576227\n",
      "STEP 11600: LOSS_BATCH = 0.114148, TIME 13939.078797\n",
      "STEP 11650: LOSS_BATCH = 0.116253, TIME 13998.470733\n",
      "STEP 11700: LOSS_BATCH = 0.115285, TIME 14058.822551\n",
      "STEP 11750: LOSS_BATCH = 0.117041, TIME 14117.291548\n",
      "STEP 11800: LOSS_BATCH = 0.111904, TIME 14176.406080\n",
      "STEP 11850: LOSS_BATCH = 0.111115, TIME 14236.335930\n",
      "STEP 11900: LOSS_BATCH = 0.118015, TIME 14295.353424\n",
      "STEP 11950: LOSS_BATCH = 0.112125, TIME 14353.820214\n",
      "STEP 12000: LOSS_BATCH = 0.114535, TIME 14413.524806\n",
      "STEP 12050: LOSS_BATCH = 0.114281, TIME 14475.634021\n",
      "STEP 12100: LOSS_BATCH = 0.111861, TIME 14536.930712\n",
      "STEP 12150: LOSS_BATCH = 0.111647, TIME 14596.779991\n",
      "STEP 12200: LOSS_BATCH = 0.110288, TIME 14657.623928\n",
      "STEP 12250: LOSS_BATCH = 0.111675, TIME 14716.505622\n",
      "STEP 12300: LOSS_BATCH = 0.110984, TIME 14776.563408\n",
      "STEP 12350: LOSS_BATCH = 0.112167, TIME 14836.310127\n",
      "STEP 12400: LOSS_BATCH = 0.116754, TIME 14899.337223\n",
      "STEP 12450: LOSS_BATCH = 0.116091, TIME 14959.044318\n",
      "STEP 12500: LOSS_BATCH = 0.110864, TIME 15022.453995\n",
      "STEP 12550: LOSS_BATCH = 0.112409, TIME 15081.784817\n",
      "STEP 12600: LOSS_BATCH = 0.110392, TIME 15140.427457\n",
      "STEP 12650: LOSS_BATCH = 0.109987, TIME 15200.292406\n",
      "STEP 12700: LOSS_BATCH = 0.112665, TIME 15260.924111\n",
      "STEP 12750: LOSS_BATCH = 0.124618, TIME 15322.217745\n",
      "STEP 12800: LOSS_BATCH = 0.113683, TIME 15383.254648\n",
      "STEP 12850: LOSS_BATCH = 0.112679, TIME 15442.817421\n",
      "STEP 12900: LOSS_BATCH = 0.118313, TIME 15501.959088\n",
      "STEP 12950: LOSS_BATCH = 0.110159, TIME 15562.072947\n",
      "STEP 13000: LOSS_BATCH = 0.110598, TIME 15624.202872\n",
      "STEP 13050: LOSS_BATCH = 0.108427, TIME 15685.934569\n",
      "STEP 13100: LOSS_BATCH = 0.117174, TIME 15743.410820\n",
      "STEP 13150: LOSS_BATCH = 0.112833, TIME 15805.203243\n",
      "STEP 13200: LOSS_BATCH = 0.114664, TIME 15864.478989\n",
      "STEP 13250: LOSS_BATCH = 0.110641, TIME 15923.274693\n",
      "STEP 13300: LOSS_BATCH = 0.107633, TIME 15983.151950\n",
      "STEP 13350: LOSS_BATCH = 0.110308, TIME 16042.575076\n",
      "STEP 13400: LOSS_BATCH = 0.117634, TIME 16104.702935\n",
      "STEP 13450: LOSS_BATCH = 0.110787, TIME 16163.952258\n",
      "STEP 13500: LOSS_BATCH = 0.114078, TIME 16225.517684\n",
      "STEP 13550: LOSS_BATCH = 0.115144, TIME 16284.686712\n",
      "STEP 13600: LOSS_BATCH = 0.117578, TIME 16344.449088\n",
      "STEP 13650: LOSS_BATCH = 0.117598, TIME 16405.398896\n",
      "STEP 13700: LOSS_BATCH = 0.109552, TIME 16467.421708\n",
      "STEP 13750: LOSS_BATCH = 0.120134, TIME 16528.004882\n",
      "STEP 13800: LOSS_BATCH = 0.115276, TIME 16587.809749\n",
      "STEP 13850: LOSS_BATCH = 0.116004, TIME 16646.853785\n",
      "STEP 13900: LOSS_BATCH = 0.111307, TIME 16706.084321\n",
      "STEP 13950: LOSS_BATCH = 0.111423, TIME 16766.147337\n",
      "STEP 14000: LOSS_BATCH = 0.113042, TIME 16826.521376\n",
      "STEP 14050: LOSS_BATCH = 0.114950, TIME 16889.364911\n",
      "STEP 14100: LOSS_BATCH = 0.111914, TIME 16949.233654\n",
      "STEP 14150: LOSS_BATCH = 0.112299, TIME 17007.676535\n",
      "STEP 14200: LOSS_BATCH = 0.117939, TIME 17067.223803\n",
      "STEP 14250: LOSS_BATCH = 0.112401, TIME 17127.081596\n",
      "STEP 14300: LOSS_BATCH = 0.113279, TIME 17188.152395\n",
      "STEP 14350: LOSS_BATCH = 0.116862, TIME 17248.094486\n",
      "STEP 14400: LOSS_BATCH = 0.107457, TIME 17311.246470\n",
      "STEP 14450: LOSS_BATCH = 0.108330, TIME 17372.382838\n",
      "STEP 14500: LOSS_BATCH = 0.110471, TIME 17432.436270\n",
      "STEP 14550: LOSS_BATCH = 0.113436, TIME 17491.662298\n",
      "STEP 14600: LOSS_BATCH = 0.113129, TIME 17551.535343\n",
      "STEP 14650: LOSS_BATCH = 0.115342, TIME 17611.256602\n",
      "STEP 14700: LOSS_BATCH = 0.115280, TIME 17673.472617\n",
      "STEP 14750: LOSS_BATCH = 0.110619, TIME 17732.404310\n",
      "STEP 14800: LOSS_BATCH = 0.112737, TIME 17791.234041\n",
      "STEP 14850: LOSS_BATCH = 0.110327, TIME 17850.982144\n",
      "STEP 14900: LOSS_BATCH = 0.114913, TIME 17909.890351\n",
      "STEP 14950: LOSS_BATCH = 0.108514, TIME 17969.786119\n",
      "STEP 15000: LOSS_BATCH = 0.111400, TIME 18030.259527\n",
      "STEP 15050: LOSS_BATCH = 0.112819, TIME 18090.370883\n",
      "STEP 15100: LOSS_BATCH = 0.110603, TIME 18150.510607\n",
      "STEP 15150: LOSS_BATCH = 0.112548, TIME 18212.572101\n",
      "STEP 15200: LOSS_BATCH = 0.110987, TIME 18272.313589\n",
      "STEP 15250: LOSS_BATCH = 0.112519, TIME 18331.368085\n",
      "STEP 15300: LOSS_BATCH = 0.109807, TIME 18393.773824\n",
      "STEP 15350: LOSS_BATCH = 0.113873, TIME 18452.912900\n",
      "STEP 15400: LOSS_BATCH = 0.110968, TIME 18513.177076\n",
      "STEP 15450: LOSS_BATCH = 0.116044, TIME 18572.094910\n",
      "STEP 15500: LOSS_BATCH = 0.113275, TIME 18632.294590\n",
      "STEP 15550: LOSS_BATCH = 0.112988, TIME 18690.881597\n",
      "STEP 15600: LOSS_BATCH = 0.107599, TIME 18750.304196\n",
      "STEP 15650: LOSS_BATCH = 0.114172, TIME 18811.179177\n",
      "STEP 15700: LOSS_BATCH = 0.113890, TIME 18870.258264\n",
      "STEP 15750: LOSS_BATCH = 0.114103, TIME 18930.121876\n",
      "STEP 15800: LOSS_BATCH = 0.109700, TIME 18988.850414\n",
      "STEP 15850: LOSS_BATCH = 0.109314, TIME 19048.501192\n",
      "STEP 15900: LOSS_BATCH = 0.115773, TIME 19106.465329\n",
      "STEP 15950: LOSS_BATCH = 0.107603, TIME 19167.896407\n",
      "STEP 16000: LOSS_BATCH = 0.111821, TIME 19228.232707\n",
      "STEP 16050: LOSS_BATCH = 0.112737, TIME 19288.338376\n",
      "STEP 16100: LOSS_BATCH = 0.112126, TIME 19347.675128\n",
      "STEP 16150: LOSS_BATCH = 0.111818, TIME 19407.861940\n",
      "STEP 16200: LOSS_BATCH = 0.109181, TIME 19470.342270\n",
      "STEP 16250: LOSS_BATCH = 0.112000, TIME 19528.280925\n",
      "STEP 16300: LOSS_BATCH = 0.111619, TIME 19587.237154\n",
      "STEP 16350: LOSS_BATCH = 0.108752, TIME 19645.815186\n",
      "STEP 16400: LOSS_BATCH = 0.116683, TIME 19709.002090\n",
      "STEP 16450: LOSS_BATCH = 0.112130, TIME 19769.241740\n",
      "STEP 16500: LOSS_BATCH = 0.113148, TIME 19829.184336\n",
      "STEP 16550: LOSS_BATCH = 0.111485, TIME 19889.155005\n",
      "STEP 16600: LOSS_BATCH = 0.108181, TIME 19948.648446\n",
      "STEP 16650: LOSS_BATCH = 0.113302, TIME 20008.833021\n",
      "STEP 16700: LOSS_BATCH = 0.117337, TIME 20067.759394\n",
      "STEP 16750: LOSS_BATCH = 0.107778, TIME 20128.197862\n",
      "STEP 16800: LOSS_BATCH = 0.114591, TIME 20188.140167\n",
      "STEP 16850: LOSS_BATCH = 0.113193, TIME 20246.760368\n",
      "STEP 16900: LOSS_BATCH = 0.111769, TIME 20308.201559\n",
      "STEP 16950: LOSS_BATCH = 0.110215, TIME 20368.275367\n",
      "STEP 17000: LOSS_BATCH = 0.111741, TIME 20431.502346\n",
      "STEP 17050: LOSS_BATCH = 0.108400, TIME 20489.903940\n",
      "STEP 17100: LOSS_BATCH = 0.115237, TIME 20551.152044\n",
      "STEP 17150: LOSS_BATCH = 0.109809, TIME 20610.751642\n",
      "STEP 17200: LOSS_BATCH = 0.115119, TIME 20670.663921\n",
      "STEP 17250: LOSS_BATCH = 0.108525, TIME 20729.963401\n",
      "STEP 17300: LOSS_BATCH = 0.109490, TIME 20792.817934\n",
      "STEP 17350: LOSS_BATCH = 0.112558, TIME 20855.531542\n",
      "STEP 17400: LOSS_BATCH = 0.116111, TIME 20916.082708\n",
      "STEP 17450: LOSS_BATCH = 0.110850, TIME 20975.860505\n",
      "STEP 17500: LOSS_BATCH = 0.107569, TIME 21034.022599\n",
      "STEP 17550: LOSS_BATCH = 0.112277, TIME 21097.010814\n",
      "STEP 17600: LOSS_BATCH = 0.109124, TIME 21156.170246\n",
      "STEP 17650: LOSS_BATCH = 0.110278, TIME 21216.125433\n",
      "STEP 17700: LOSS_BATCH = 0.108746, TIME 21277.826550\n",
      "STEP 17750: LOSS_BATCH = 0.108597, TIME 21339.462844\n",
      "STEP 17800: LOSS_BATCH = 0.114094, TIME 21399.583216\n",
      "STEP 17850: LOSS_BATCH = 0.110981, TIME 21460.123140\n",
      "STEP 17900: LOSS_BATCH = 0.110346, TIME 21519.223149\n",
      "STEP 17950: LOSS_BATCH = 0.107660, TIME 21579.036946\n",
      "STEP 18000: LOSS_BATCH = 0.116005, TIME 21638.718868\n",
      "STEP 18050: LOSS_BATCH = 0.112516, TIME 21696.682542\n",
      "STEP 18100: LOSS_BATCH = 0.114970, TIME 21756.298294\n",
      "STEP 18150: LOSS_BATCH = 0.108151, TIME 21819.344471\n",
      "STEP 18200: LOSS_BATCH = 0.116153, TIME 21880.388419\n",
      "STEP 18250: LOSS_BATCH = 0.112652, TIME 21940.221847\n",
      "STEP 18300: LOSS_BATCH = 0.118952, TIME 22000.213252\n",
      "STEP 18350: LOSS_BATCH = 0.113348, TIME 22061.034795\n",
      "STEP 18400: LOSS_BATCH = 0.114520, TIME 22121.266834\n",
      "STEP 18450: LOSS_BATCH = 0.111026, TIME 22182.317379\n",
      "STEP 18500: LOSS_BATCH = 0.114552, TIME 22240.119890\n",
      "STEP 18550: LOSS_BATCH = 0.110519, TIME 22301.093670\n",
      "STEP 18600: LOSS_BATCH = 0.115422, TIME 22360.691514\n",
      "STEP 18650: LOSS_BATCH = 0.112737, TIME 22420.745424\n",
      "STEP 18700: LOSS_BATCH = 0.109834, TIME 22483.961874\n",
      "STEP 18750: LOSS_BATCH = 0.110404, TIME 22543.211437\n",
      "STEP 18800: LOSS_BATCH = 0.118940, TIME 22601.402236\n",
      "STEP 18850: LOSS_BATCH = 0.109780, TIME 22663.898124\n",
      "STEP 18900: LOSS_BATCH = 0.109374, TIME 22722.482883\n",
      "STEP 18950: LOSS_BATCH = 0.110434, TIME 22782.921617\n",
      "STEP 19000: LOSS_BATCH = 0.114315, TIME 22845.819815\n",
      "STEP 19050: LOSS_BATCH = 0.112990, TIME 22904.432953\n",
      "STEP 19100: LOSS_BATCH = 0.112320, TIME 22966.374206\n",
      "STEP 19150: LOSS_BATCH = 0.113530, TIME 23024.573027\n",
      "STEP 19200: LOSS_BATCH = 0.110652, TIME 23085.108249\n",
      "STEP 19250: LOSS_BATCH = 0.107648, TIME 23148.289063\n",
      "STEP 19300: LOSS_BATCH = 0.110829, TIME 23206.479048\n",
      "STEP 19350: LOSS_BATCH = 0.109856, TIME 23268.063215\n",
      "STEP 19400: LOSS_BATCH = 0.113782, TIME 23327.468392\n",
      "STEP 19450: LOSS_BATCH = 0.112264, TIME 23385.410710\n",
      "STEP 19500: LOSS_BATCH = 0.112056, TIME 23447.381902\n",
      "STEP 19550: LOSS_BATCH = 0.116886, TIME 23507.243989\n",
      "STEP 19600: LOSS_BATCH = 0.113329, TIME 23567.610640\n",
      "STEP 19650: LOSS_BATCH = 0.111593, TIME 23628.192139\n",
      "STEP 19700: LOSS_BATCH = 0.113766, TIME 23687.826284\n",
      "STEP 19750: LOSS_BATCH = 0.114475, TIME 23747.810297\n",
      "STEP 19800: LOSS_BATCH = 0.106671, TIME 23810.018034\n",
      "STEP 19850: LOSS_BATCH = 0.116423, TIME 23870.254324\n",
      "STEP 19900: LOSS_BATCH = 0.113881, TIME 23929.087603\n",
      "STEP 19950: LOSS_BATCH = 0.117021, TIME 23990.502587\n",
      "STEP 20000: LOSS_BATCH = 0.112874, TIME 24049.761607\n",
      "STEP 20050: LOSS_BATCH = 0.112320, TIME 24110.858696\n",
      "STEP 20100: LOSS_BATCH = 0.115345, TIME 24170.231802\n",
      "STEP 20150: LOSS_BATCH = 0.108618, TIME 24228.745702\n",
      "STEP 20200: LOSS_BATCH = 0.114064, TIME 24289.488231\n",
      "STEP 20250: LOSS_BATCH = 0.110101, TIME 24348.682600\n",
      "STEP 20300: LOSS_BATCH = 0.115146, TIME 24407.442358\n",
      "STEP 20350: LOSS_BATCH = 0.117340, TIME 24467.339056\n",
      "STEP 20400: LOSS_BATCH = 0.107021, TIME 24526.811722\n",
      "STEP 20450: LOSS_BATCH = 0.112347, TIME 24586.700764\n",
      "STEP 20500: LOSS_BATCH = 0.112323, TIME 24879.806455\n",
      "STEP 20550: LOSS_BATCH = 0.110846, TIME 24935.908214\n",
      "STEP 20600: LOSS_BATCH = 0.110091, TIME 24992.995363\n",
      "STEP 20650: LOSS_BATCH = 0.110789, TIME 25050.131280\n",
      "STEP 20700: LOSS_BATCH = 0.112827, TIME 25109.902348\n",
      "STEP 20750: LOSS_BATCH = 0.114725, TIME 25167.867822\n",
      "STEP 20800: LOSS_BATCH = 0.110044, TIME 25225.916835\n",
      "STEP 20850: LOSS_BATCH = 0.108204, TIME 25284.352649\n",
      "STEP 20900: LOSS_BATCH = 0.109774, TIME 25343.478880\n",
      "STEP 20950: LOSS_BATCH = 0.113587, TIME 25403.833418\n",
      "STEP 21000: LOSS_BATCH = 0.111250, TIME 25462.205593\n",
      "STEP 21050: LOSS_BATCH = 0.109287, TIME 25521.370117\n",
      "STEP 21100: LOSS_BATCH = 0.109194, TIME 25580.769609\n",
      "STEP 21150: LOSS_BATCH = 0.109833, TIME 25641.109922\n",
      "STEP 21200: LOSS_BATCH = 0.112483, TIME 25703.440645\n",
      "STEP 21250: LOSS_BATCH = 0.115418, TIME 25762.217830\n",
      "STEP 21300: LOSS_BATCH = 0.115487, TIME 25822.475345\n",
      "STEP 21350: LOSS_BATCH = 0.109896, TIME 25882.634862\n",
      "STEP 21400: LOSS_BATCH = 0.111865, TIME 25941.661033\n",
      "STEP 21450: LOSS_BATCH = 0.114342, TIME 26003.637549\n",
      "STEP 21500: LOSS_BATCH = 0.112122, TIME 26062.659212\n",
      "STEP 21550: LOSS_BATCH = 0.110851, TIME 26121.999104\n",
      "STEP 21600: LOSS_BATCH = 0.111048, TIME 26180.468371\n",
      "STEP 21650: LOSS_BATCH = 0.115164, TIME 26241.679966\n",
      "STEP 21700: LOSS_BATCH = 0.114625, TIME 26303.167928\n",
      "STEP 21750: LOSS_BATCH = 0.109623, TIME 26365.740233\n",
      "STEP 21800: LOSS_BATCH = 0.116313, TIME 26424.918851\n",
      "STEP 21850: LOSS_BATCH = 0.111311, TIME 26485.029142\n",
      "STEP 21900: LOSS_BATCH = 0.111169, TIME 26544.705631\n",
      "STEP 21950: LOSS_BATCH = 0.112401, TIME 26604.256840\n",
      "STEP 22000: LOSS_BATCH = 0.110026, TIME 26664.373429\n",
      "STEP 22050: LOSS_BATCH = 0.112461, TIME 26723.596488\n",
      "STEP 22100: LOSS_BATCH = 0.109383, TIME 26782.501679\n",
      "STEP 22150: LOSS_BATCH = 0.110158, TIME 26840.841929\n",
      "STEP 22200: LOSS_BATCH = 0.111261, TIME 26900.269134\n",
      "STEP 22250: LOSS_BATCH = 0.108870, TIME 26959.817233\n",
      "STEP 22300: LOSS_BATCH = 0.111156, TIME 27019.291078\n",
      "STEP 22350: LOSS_BATCH = 0.109791, TIME 27081.787416\n",
      "STEP 22400: LOSS_BATCH = 0.111371, TIME 27141.202045\n",
      "STEP 22450: LOSS_BATCH = 0.115378, TIME 27202.859670\n",
      "STEP 22500: LOSS_BATCH = 0.108672, TIME 27262.169454\n",
      "STEP 22550: LOSS_BATCH = 0.111972, TIME 27320.472251\n",
      "STEP 22600: LOSS_BATCH = 0.114333, TIME 27382.031690\n",
      "STEP 22650: LOSS_BATCH = 0.113612, TIME 27443.041120\n",
      "STEP 22700: LOSS_BATCH = 0.115398, TIME 27504.329013\n",
      "STEP 22750: LOSS_BATCH = 0.114268, TIME 27564.310401\n",
      "STEP 22800: LOSS_BATCH = 0.116737, TIME 27623.762261\n",
      "STEP 22850: LOSS_BATCH = 0.111058, TIME 27683.072500\n",
      "STEP 22900: LOSS_BATCH = 0.109503, TIME 27742.254442\n",
      "STEP 22950: LOSS_BATCH = 0.110585, TIME 27802.391727\n",
      "STEP 23000: LOSS_BATCH = 0.107327, TIME 27861.998290\n",
      "STEP 23050: LOSS_BATCH = 0.112505, TIME 27921.859048\n",
      "STEP 23100: LOSS_BATCH = 0.112775, TIME 27981.980580\n",
      "STEP 23150: LOSS_BATCH = 0.113828, TIME 28041.558086\n",
      "STEP 23200: LOSS_BATCH = 0.110169, TIME 28100.266294\n",
      "STEP 23250: LOSS_BATCH = 0.118208, TIME 28158.731228\n",
      "STEP 23300: LOSS_BATCH = 0.116107, TIME 28217.928791\n",
      "STEP 23350: LOSS_BATCH = 0.109233, TIME 28277.582908\n",
      "STEP 23400: LOSS_BATCH = 0.112923, TIME 28336.250157\n",
      "STEP 23450: LOSS_BATCH = 0.111632, TIME 28396.729528\n",
      "STEP 23500: LOSS_BATCH = 0.113027, TIME 28456.773440\n",
      "STEP 23550: LOSS_BATCH = 0.109134, TIME 28516.014955\n",
      "STEP 23600: LOSS_BATCH = 0.108891, TIME 28574.469534\n",
      "STEP 23650: LOSS_BATCH = 0.114474, TIME 28633.024977\n",
      "STEP 23700: LOSS_BATCH = 0.108621, TIME 28692.363095\n",
      "STEP 23750: LOSS_BATCH = 0.110617, TIME 28753.136709\n",
      "STEP 23800: LOSS_BATCH = 0.109166, TIME 28811.036619\n",
      "STEP 23850: LOSS_BATCH = 0.107628, TIME 28870.697572\n",
      "STEP 23900: LOSS_BATCH = 0.113663, TIME 28930.047956\n",
      "STEP 23950: LOSS_BATCH = 0.112814, TIME 28988.624274\n",
      "STEP 24000: LOSS_BATCH = 0.113051, TIME 29048.733719\n",
      "STEP 24050: LOSS_BATCH = 0.106942, TIME 29108.379550\n",
      "STEP 24100: LOSS_BATCH = 0.110400, TIME 29168.112062\n",
      "STEP 24150: LOSS_BATCH = 0.110299, TIME 29227.473529\n",
      "STEP 24200: LOSS_BATCH = 0.111769, TIME 29290.733049\n",
      "STEP 24250: LOSS_BATCH = 0.108319, TIME 29350.028543\n",
      "STEP 24300: LOSS_BATCH = 0.112754, TIME 29410.180651\n",
      "STEP 24350: LOSS_BATCH = 0.111957, TIME 29474.011696\n",
      "STEP 24400: LOSS_BATCH = 0.109047, TIME 29533.760385\n",
      "STEP 24450: LOSS_BATCH = 0.113011, TIME 29593.089794\n",
      "STEP 24500: LOSS_BATCH = 0.113681, TIME 29651.466474\n",
      "STEP 24550: LOSS_BATCH = 0.112868, TIME 29711.571932\n",
      "STEP 24600: LOSS_BATCH = 0.113085, TIME 29771.630526\n",
      "STEP 24650: LOSS_BATCH = 0.115357, TIME 29832.362064\n",
      "STEP 24700: LOSS_BATCH = 0.110282, TIME 29892.989724\n",
      "STEP 24750: LOSS_BATCH = 0.110817, TIME 29953.044132\n",
      "STEP 24800: LOSS_BATCH = 0.112616, TIME 30013.185037\n",
      "STEP 24850: LOSS_BATCH = 0.114368, TIME 30073.391972\n",
      "STEP 24900: LOSS_BATCH = 0.111766, TIME 30132.046669\n",
      "STEP 24950: LOSS_BATCH = 0.112000, TIME 30191.205468\n",
      "STEP 25000: LOSS_BATCH = 0.111920, TIME 30250.633543\n",
      "STEP 25050: LOSS_BATCH = 0.114695, TIME 30310.281721\n",
      "STEP 25100: LOSS_BATCH = 0.111806, TIME 30368.351033\n",
      "STEP 25150: LOSS_BATCH = 0.114982, TIME 30427.825446\n",
      "STEP 25200: LOSS_BATCH = 0.111522, TIME 30487.966762\n",
      "STEP 25250: LOSS_BATCH = 0.113519, TIME 30548.904256\n",
      "STEP 25300: LOSS_BATCH = 0.109089, TIME 30607.915174\n",
      "STEP 25350: LOSS_BATCH = 0.114141, TIME 30668.167138\n",
      "STEP 25400: LOSS_BATCH = 0.107783, TIME 30730.222565\n",
      "STEP 25450: LOSS_BATCH = 0.114914, TIME 30790.163017\n",
      "STEP 25500: LOSS_BATCH = 0.115190, TIME 30848.873493\n",
      "STEP 25550: LOSS_BATCH = 0.112756, TIME 30908.297269\n",
      "STEP 25600: LOSS_BATCH = 0.107728, TIME 30971.648369\n",
      "STEP 25650: LOSS_BATCH = 0.107234, TIME 31032.172430\n",
      "STEP 25700: LOSS_BATCH = 0.111422, TIME 31092.779332\n",
      "STEP 25750: LOSS_BATCH = 0.109083, TIME 31154.737571\n",
      "STEP 25800: LOSS_BATCH = 0.116213, TIME 31213.384032\n",
      "STEP 25850: LOSS_BATCH = 0.111013, TIME 31276.328109\n",
      "STEP 25900: LOSS_BATCH = 0.111860, TIME 31333.793730\n",
      "STEP 25950: LOSS_BATCH = 0.109539, TIME 31393.638332\n",
      "STEP 26000: LOSS_BATCH = 0.109652, TIME 31452.203152\n",
      "STEP 26050: LOSS_BATCH = 0.113741, TIME 31510.866058\n",
      "STEP 26100: LOSS_BATCH = 0.112536, TIME 31573.777385\n",
      "STEP 26150: LOSS_BATCH = 0.111429, TIME 31632.526066\n",
      "STEP 26200: LOSS_BATCH = 0.107763, TIME 31691.942600\n",
      "STEP 26250: LOSS_BATCH = 0.110776, TIME 31751.739409\n",
      "STEP 26300: LOSS_BATCH = 0.110021, TIME 31814.416005\n",
      "STEP 26350: LOSS_BATCH = 0.111474, TIME 31873.668462\n",
      "STEP 26400: LOSS_BATCH = 0.108896, TIME 31933.362118\n",
      "STEP 26450: LOSS_BATCH = 0.114987, TIME 31993.641525\n",
      "STEP 26500: LOSS_BATCH = 0.112828, TIME 32052.128082\n",
      "STEP 26550: LOSS_BATCH = 0.112840, TIME 32112.349457\n",
      "STEP 26600: LOSS_BATCH = 0.110520, TIME 32171.732983\n",
      "STEP 26650: LOSS_BATCH = 0.108382, TIME 32230.571467\n",
      "STEP 26700: LOSS_BATCH = 0.113558, TIME 32291.898907\n",
      "STEP 26750: LOSS_BATCH = 0.110935, TIME 32352.253094\n",
      "STEP 26800: LOSS_BATCH = 0.117029, TIME 32411.995030\n",
      "STEP 26850: LOSS_BATCH = 0.111396, TIME 32471.486259\n",
      "STEP 26900: LOSS_BATCH = 0.111585, TIME 32531.686352\n",
      "STEP 26950: LOSS_BATCH = 0.114660, TIME 32592.069841\n",
      "STEP 27000: LOSS_BATCH = 0.116404, TIME 32652.149558\n",
      "STEP 27050: LOSS_BATCH = 0.106710, TIME 32713.852063\n",
      "STEP 27100: LOSS_BATCH = 0.109341, TIME 32775.279265\n",
      "STEP 27150: LOSS_BATCH = 0.109031, TIME 32833.982640\n",
      "STEP 27200: LOSS_BATCH = 0.107889, TIME 32893.241983\n",
      "STEP 27250: LOSS_BATCH = 0.110513, TIME 32952.285205\n",
      "STEP 27300: LOSS_BATCH = 0.112901, TIME 33011.637908\n",
      "STEP 27350: LOSS_BATCH = 0.112464, TIME 33071.237571\n",
      "STEP 27400: LOSS_BATCH = 0.107236, TIME 33133.333534\n",
      "STEP 27450: LOSS_BATCH = 0.114410, TIME 33193.139632\n",
      "STEP 27500: LOSS_BATCH = 0.113610, TIME 33253.398998\n",
      "STEP 27550: LOSS_BATCH = 0.111078, TIME 33313.660272\n",
      "STEP 27600: LOSS_BATCH = 0.110547, TIME 33374.163440\n",
      "STEP 27650: LOSS_BATCH = 0.108929, TIME 33434.088931\n",
      "STEP 27700: LOSS_BATCH = 0.106885, TIME 33493.479456\n",
      "STEP 27750: LOSS_BATCH = 0.112512, TIME 33552.981529\n",
      "STEP 27800: LOSS_BATCH = 0.110838, TIME 33612.886318\n",
      "STEP 27850: LOSS_BATCH = 0.114652, TIME 33672.863221\n",
      "STEP 27900: LOSS_BATCH = 0.109580, TIME 33731.098436\n",
      "STEP 27950: LOSS_BATCH = 0.111775, TIME 33793.731987\n",
      "STEP 28000: LOSS_BATCH = 0.111904, TIME 33853.168073\n",
      "STEP 28050: LOSS_BATCH = 0.108360, TIME 33912.917098\n",
      "STEP 28100: LOSS_BATCH = 0.111489, TIME 33972.421575\n",
      "STEP 28150: LOSS_BATCH = 0.110600, TIME 34031.911486\n",
      "STEP 28200: LOSS_BATCH = 0.107992, TIME 34095.259163\n",
      "STEP 28250: LOSS_BATCH = 0.110643, TIME 34153.982130\n",
      "STEP 28300: LOSS_BATCH = 0.115149, TIME 34214.332825\n",
      "STEP 28350: LOSS_BATCH = 0.111131, TIME 34275.249115\n",
      "STEP 28400: LOSS_BATCH = 0.110220, TIME 34338.156239\n",
      "STEP 28450: LOSS_BATCH = 0.108962, TIME 34397.557962\n",
      "STEP 28500: LOSS_BATCH = 0.116192, TIME 34455.869130\n",
      "STEP 28550: LOSS_BATCH = 0.110203, TIME 34516.010756\n",
      "STEP 28600: LOSS_BATCH = 0.108999, TIME 34578.533929\n",
      "STEP 28650: LOSS_BATCH = 0.109999, TIME 34641.124415\n",
      "STEP 28700: LOSS_BATCH = 0.108459, TIME 34701.325602\n",
      "STEP 28750: LOSS_BATCH = 0.116834, TIME 34760.512250\n",
      "STEP 28800: LOSS_BATCH = 0.113179, TIME 34822.586012\n",
      "STEP 28850: LOSS_BATCH = 0.113254, TIME 34886.239666\n",
      "STEP 28900: LOSS_BATCH = 0.112036, TIME 34946.549618\n",
      "STEP 28950: LOSS_BATCH = 0.110784, TIME 35005.920152\n",
      "STEP 29000: LOSS_BATCH = 0.112843, TIME 35065.136000\n",
      "STEP 29050: LOSS_BATCH = 0.108397, TIME 35124.196037\n",
      "STEP 29100: LOSS_BATCH = 0.107994, TIME 35184.351002\n",
      "STEP 29150: LOSS_BATCH = 0.112826, TIME 35244.422870\n",
      "STEP 29200: LOSS_BATCH = 0.111735, TIME 35306.937852\n",
      "STEP 29250: LOSS_BATCH = 0.109365, TIME 35366.616562\n",
      "STEP 29300: LOSS_BATCH = 0.113329, TIME 35426.190136\n",
      "STEP 29350: LOSS_BATCH = 0.111092, TIME 35485.215698\n",
      "STEP 29400: LOSS_BATCH = 0.111532, TIME 35546.203744\n",
      "STEP 29450: LOSS_BATCH = 0.110599, TIME 35604.534224\n",
      "STEP 29500: LOSS_BATCH = 0.111603, TIME 35665.266643\n",
      "STEP 29550: LOSS_BATCH = 0.106961, TIME 35725.875019\n",
      "STEP 29600: LOSS_BATCH = 0.112922, TIME 35787.549726\n",
      "STEP 29650: LOSS_BATCH = 0.109877, TIME 35847.133416\n",
      "STEP 29700: LOSS_BATCH = 0.109773, TIME 35909.623840\n",
      "STEP 29750: LOSS_BATCH = 0.115454, TIME 35970.261685\n",
      "STEP 29800: LOSS_BATCH = 0.110226, TIME 36031.203254\n",
      "STEP 29850: LOSS_BATCH = 0.116456, TIME 36094.456987\n",
      "STEP 29900: LOSS_BATCH = 0.109047, TIME 36155.466780\n",
      "STEP 29950: LOSS_BATCH = 0.109706, TIME 36215.862202\n",
      "STEP 30000: LOSS_BATCH = 0.111906, TIME 36276.705100\n",
      "STEP 30050: LOSS_BATCH = 0.110549, TIME 36337.188058\n",
      "STEP 30100: LOSS_BATCH = 0.109240, TIME 36397.417126\n",
      "STEP 30150: LOSS_BATCH = 0.110737, TIME 36456.402508\n",
      "STEP 30200: LOSS_BATCH = 0.110398, TIME 36517.433164\n",
      "STEP 30250: LOSS_BATCH = 0.107720, TIME 36576.754991\n",
      "STEP 30300: LOSS_BATCH = 0.112993, TIME 36637.209398\n",
      "STEP 30350: LOSS_BATCH = 0.111138, TIME 36697.844132\n",
      "STEP 30400: LOSS_BATCH = 0.109300, TIME 36761.271710\n",
      "STEP 30450: LOSS_BATCH = 0.110493, TIME 36820.915379\n",
      "STEP 30500: LOSS_BATCH = 0.115141, TIME 36881.065552\n",
      "STEP 30550: LOSS_BATCH = 0.113878, TIME 36940.101466\n",
      "STEP 30600: LOSS_BATCH = 0.107509, TIME 36999.910319\n",
      "STEP 30650: LOSS_BATCH = 0.107927, TIME 37059.339486\n",
      "STEP 30700: LOSS_BATCH = 0.112447, TIME 37119.386085\n",
      "STEP 30750: LOSS_BATCH = 0.111438, TIME 37179.701615\n",
      "STEP 30800: LOSS_BATCH = 0.109782, TIME 37242.921317\n",
      "STEP 30850: LOSS_BATCH = 0.110031, TIME 37306.247831\n",
      "STEP 30900: LOSS_BATCH = 0.108099, TIME 37368.704413\n",
      "STEP 30950: LOSS_BATCH = 0.111246, TIME 37427.831325\n",
      "STEP 31000: LOSS_BATCH = 0.114340, TIME 37486.806909\n",
      "STEP 31050: LOSS_BATCH = 0.115231, TIME 37546.524832\n",
      "STEP 31100: LOSS_BATCH = 0.114923, TIME 37606.834204\n",
      "STEP 31150: LOSS_BATCH = 0.112221, TIME 37665.650442\n",
      "STEP 31200: LOSS_BATCH = 0.112498, TIME 37726.537044\n",
      "STEP 31250: LOSS_BATCH = 0.115230, TIME 37784.478544\n",
      "STEP 31300: LOSS_BATCH = 0.111908, TIME 37847.790879\n",
      "STEP 31350: LOSS_BATCH = 0.115022, TIME 37907.445553\n",
      "STEP 31400: LOSS_BATCH = 0.110315, TIME 37969.308618\n",
      "STEP 31450: LOSS_BATCH = 0.108748, TIME 38029.336858\n",
      "STEP 31500: LOSS_BATCH = 0.114820, TIME 38091.029429\n",
      "STEP 31550: LOSS_BATCH = 0.111000, TIME 38150.540311\n",
      "STEP 31600: LOSS_BATCH = 0.111517, TIME 38211.951031\n",
      "STEP 31650: LOSS_BATCH = 0.110184, TIME 38272.405878\n",
      "STEP 31700: LOSS_BATCH = 0.115528, TIME 38332.611479\n",
      "STEP 31750: LOSS_BATCH = 0.114259, TIME 38392.420252\n",
      "STEP 31800: LOSS_BATCH = 0.108335, TIME 38451.653254\n",
      "STEP 31850: LOSS_BATCH = 0.109886, TIME 38511.826056\n",
      "STEP 31900: LOSS_BATCH = 0.112069, TIME 38570.436047\n",
      "STEP 31950: LOSS_BATCH = 0.113561, TIME 38630.400252\n",
      "STEP 32000: LOSS_BATCH = 0.110660, TIME 38690.644603\n",
      "STEP 32050: LOSS_BATCH = 0.110635, TIME 38751.767526\n",
      "STEP 32100: LOSS_BATCH = 0.113315, TIME 38811.245206\n",
      "STEP 32150: LOSS_BATCH = 0.110177, TIME 38874.082232\n",
      "STEP 32200: LOSS_BATCH = 0.110653, TIME 38937.238696\n",
      "STEP 32250: LOSS_BATCH = 0.110615, TIME 38999.416156\n",
      "STEP 32300: LOSS_BATCH = 0.112319, TIME 39059.935846\n",
      "STEP 32350: LOSS_BATCH = 0.114525, TIME 39119.940273\n",
      "STEP 32400: LOSS_BATCH = 0.107345, TIME 39181.152407\n",
      "STEP 32450: LOSS_BATCH = 0.108032, TIME 39241.849260\n",
      "STEP 32500: LOSS_BATCH = 0.114491, TIME 39303.025625\n",
      "STEP 32550: LOSS_BATCH = 0.108006, TIME 39363.095611\n",
      "STEP 32600: LOSS_BATCH = 0.111619, TIME 39423.040920\n",
      "STEP 32650: LOSS_BATCH = 0.113241, TIME 39483.481042\n",
      "STEP 32700: LOSS_BATCH = 0.107732, TIME 39544.809462\n",
      "STEP 32750: LOSS_BATCH = 0.115419, TIME 39605.375443\n",
      "STEP 32800: LOSS_BATCH = 0.112324, TIME 39667.208968\n",
      "STEP 32850: LOSS_BATCH = 0.109061, TIME 39730.439274\n",
      "STEP 32900: LOSS_BATCH = 0.107909, TIME 39791.036628\n",
      "STEP 32950: LOSS_BATCH = 0.109997, TIME 39850.270075\n",
      "STEP 33000: LOSS_BATCH = 0.109604, TIME 39912.693636\n",
      "STEP 33050: LOSS_BATCH = 0.111715, TIME 39972.002222\n",
      "STEP 33100: LOSS_BATCH = 0.110418, TIME 40034.764860\n",
      "STEP 33150: LOSS_BATCH = 0.115480, TIME 40095.033109\n",
      "STEP 33200: LOSS_BATCH = 0.110803, TIME 40155.316601\n",
      "STEP 33250: LOSS_BATCH = 0.112052, TIME 40218.302792\n",
      "STEP 33300: LOSS_BATCH = 0.112709, TIME 40277.793787\n",
      "STEP 33350: LOSS_BATCH = 0.113820, TIME 40341.123736\n",
      "STEP 33400: LOSS_BATCH = 0.112550, TIME 40401.032821\n",
      "STEP 33450: LOSS_BATCH = 0.108156, TIME 40461.607213\n",
      "STEP 33500: LOSS_BATCH = 0.113806, TIME 40520.997684\n",
      "STEP 33550: LOSS_BATCH = 0.112951, TIME 40584.209259\n",
      "STEP 33600: LOSS_BATCH = 0.111941, TIME 40644.339919\n",
      "STEP 33650: LOSS_BATCH = 0.110487, TIME 40703.278166\n",
      "STEP 33700: LOSS_BATCH = 0.111242, TIME 40766.213142\n",
      "STEP 33750: LOSS_BATCH = 0.107651, TIME 40828.558872\n",
      "STEP 33800: LOSS_BATCH = 0.107769, TIME 40887.760958\n",
      "STEP 33850: LOSS_BATCH = 0.110954, TIME 40949.904987\n",
      "STEP 33900: LOSS_BATCH = 0.111616, TIME 41010.415344\n",
      "STEP 33950: LOSS_BATCH = 0.110303, TIME 41069.476605\n",
      "STEP 34000: LOSS_BATCH = 0.110691, TIME 41129.985996\n",
      "STEP 34050: LOSS_BATCH = 0.113244, TIME 41190.883709\n",
      "STEP 34100: LOSS_BATCH = 0.113289, TIME 41252.324937\n",
      "STEP 34150: LOSS_BATCH = 0.110317, TIME 41314.072505\n",
      "STEP 34200: LOSS_BATCH = 0.117094, TIME 41377.472193\n",
      "STEP 34250: LOSS_BATCH = 0.108902, TIME 41437.372388\n",
      "STEP 34300: LOSS_BATCH = 0.113090, TIME 41497.691064\n",
      "STEP 34350: LOSS_BATCH = 0.111399, TIME 41558.406959\n",
      "STEP 34400: LOSS_BATCH = 0.112239, TIME 41621.411017\n",
      "STEP 34450: LOSS_BATCH = 0.108162, TIME 41685.034123\n",
      "STEP 34500: LOSS_BATCH = 0.107257, TIME 41745.286592\n",
      "STEP 34550: LOSS_BATCH = 0.109833, TIME 41806.523882\n",
      "STEP 34600: LOSS_BATCH = 0.113449, TIME 41866.260829\n",
      "STEP 34650: LOSS_BATCH = 0.108007, TIME 41927.671563\n",
      "STEP 34700: LOSS_BATCH = 0.113309, TIME 41991.096991\n",
      "STEP 34750: LOSS_BATCH = 0.114978, TIME 42050.044833\n",
      "STEP 34800: LOSS_BATCH = 0.108181, TIME 42111.297459\n",
      "STEP 34850: LOSS_BATCH = 0.112104, TIME 42171.011209\n",
      "STEP 34900: LOSS_BATCH = 0.110382, TIME 42234.830060\n",
      "STEP 34950: LOSS_BATCH = 0.107090, TIME 42294.444539\n",
      "STEP 35000: LOSS_BATCH = 0.108519, TIME 42356.260658\n",
      "STEP 35050: LOSS_BATCH = 0.110823, TIME 42419.461849\n",
      "STEP 35100: LOSS_BATCH = 0.110746, TIME 42479.161804\n",
      "STEP 35150: LOSS_BATCH = 0.114460, TIME 42538.365799\n",
      "STEP 35200: LOSS_BATCH = 0.117167, TIME 42601.461303\n",
      "STEP 35250: LOSS_BATCH = 0.109828, TIME 42661.030561\n",
      "STEP 35300: LOSS_BATCH = 0.110761, TIME 42722.952127\n",
      "STEP 35350: LOSS_BATCH = 0.111861, TIME 42782.420825\n",
      "STEP 35400: LOSS_BATCH = 0.107020, TIME 42843.062715\n",
      "STEP 35450: LOSS_BATCH = 0.108801, TIME 42903.358635\n",
      "STEP 35500: LOSS_BATCH = 0.109049, TIME 42963.239775\n",
      "STEP 35550: LOSS_BATCH = 0.113920, TIME 43026.144101\n",
      "STEP 35600: LOSS_BATCH = 0.110800, TIME 43088.253111\n",
      "STEP 35650: LOSS_BATCH = 0.111043, TIME 43148.799565\n",
      "STEP 35700: LOSS_BATCH = 0.109953, TIME 43208.884045\n",
      "STEP 35750: LOSS_BATCH = 0.113880, TIME 43267.908248\n",
      "STEP 35800: LOSS_BATCH = 0.114846, TIME 43328.795715\n",
      "STEP 35850: LOSS_BATCH = 0.114955, TIME 43389.630221\n",
      "STEP 35900: LOSS_BATCH = 0.111801, TIME 43450.644348\n",
      "STEP 35950: LOSS_BATCH = 0.111443, TIME 43510.982405\n",
      "STEP 36000: LOSS_BATCH = 0.108552, TIME 43572.631804\n",
      "STEP 36050: LOSS_BATCH = 0.109450, TIME 43632.534756\n",
      "STEP 36100: LOSS_BATCH = 0.108706, TIME 43694.469246\n",
      "STEP 36150: LOSS_BATCH = 0.116067, TIME 43757.666864\n",
      "STEP 36200: LOSS_BATCH = 0.109548, TIME 43817.786338\n",
      "STEP 36250: LOSS_BATCH = 0.110066, TIME 43877.545143\n",
      "STEP 36300: LOSS_BATCH = 0.110352, TIME 43938.839597\n",
      "STEP 36350: LOSS_BATCH = 0.109353, TIME 43998.630119\n",
      "STEP 36400: LOSS_BATCH = 0.110235, TIME 44059.997422\n",
      "STEP 36450: LOSS_BATCH = 0.115837, TIME 44122.694000\n",
      "STEP 36500: LOSS_BATCH = 0.113493, TIME 44183.537870\n",
      "STEP 36550: LOSS_BATCH = 0.112243, TIME 44243.587120\n",
      "STEP 36600: LOSS_BATCH = 0.108483, TIME 44305.064318\n",
      "STEP 36650: LOSS_BATCH = 0.108930, TIME 44365.606593\n",
      "STEP 36700: LOSS_BATCH = 0.113526, TIME 44425.354991\n",
      "STEP 36750: LOSS_BATCH = 0.106742, TIME 44485.573048\n",
      "STEP 36800: LOSS_BATCH = 0.108761, TIME 44546.650271\n",
      "STEP 36850: LOSS_BATCH = 0.112195, TIME 44608.466316\n",
      "STEP 36900: LOSS_BATCH = 0.108029, TIME 44672.583973\n",
      "STEP 36950: LOSS_BATCH = 0.113971, TIME 44731.805834\n",
      "STEP 37000: LOSS_BATCH = 0.114567, TIME 44792.557636\n",
      "STEP 37050: LOSS_BATCH = 0.114460, TIME 44853.011373\n",
      "STEP 37100: LOSS_BATCH = 0.109936, TIME 44912.040286\n",
      "STEP 37150: LOSS_BATCH = 0.108908, TIME 44972.761291\n",
      "STEP 37200: LOSS_BATCH = 0.112492, TIME 45032.679944\n",
      "STEP 37250: LOSS_BATCH = 0.107886, TIME 45095.960129\n",
      "STEP 37300: LOSS_BATCH = 0.110228, TIME 45156.345394\n",
      "STEP 37350: LOSS_BATCH = 0.111119, TIME 45217.726887\n",
      "STEP 37400: LOSS_BATCH = 0.115204, TIME 45281.265422\n",
      "STEP 37450: LOSS_BATCH = 0.111988, TIME 45341.053184\n",
      "STEP 37500: LOSS_BATCH = 0.109625, TIME 45400.426847\n",
      "STEP 37550: LOSS_BATCH = 0.111042, TIME 45464.044778\n",
      "STEP 37600: LOSS_BATCH = 0.110909, TIME 45523.752804\n",
      "STEP 37650: LOSS_BATCH = 0.110801, TIME 45583.294232\n",
      "STEP 37700: LOSS_BATCH = 0.109943, TIME 45642.749094\n",
      "STEP 37750: LOSS_BATCH = 0.111262, TIME 45702.699633\n",
      "STEP 37800: LOSS_BATCH = 0.109088, TIME 45762.092194\n",
      "STEP 37850: LOSS_BATCH = 0.108077, TIME 45823.089564\n",
      "STEP 37900: LOSS_BATCH = 0.110483, TIME 45882.564088\n",
      "STEP 37950: LOSS_BATCH = 0.112778, TIME 45943.630609\n",
      "STEP 38000: LOSS_BATCH = 0.110981, TIME 46004.939599\n",
      "STEP 38050: LOSS_BATCH = 0.107656, TIME 46065.352380\n",
      "STEP 38100: LOSS_BATCH = 0.109148, TIME 46125.501895\n",
      "STEP 38150: LOSS_BATCH = 0.112397, TIME 46186.082697\n",
      "STEP 38200: LOSS_BATCH = 0.112669, TIME 46247.222752\n",
      "STEP 38250: LOSS_BATCH = 0.110335, TIME 46306.290904\n",
      "STEP 38300: LOSS_BATCH = 0.111321, TIME 46369.007350\n",
      "STEP 38350: LOSS_BATCH = 0.111300, TIME 46429.555286\n",
      "STEP 38400: LOSS_BATCH = 0.108026, TIME 46491.047286\n",
      "STEP 38450: LOSS_BATCH = 0.108715, TIME 46552.331019\n",
      "STEP 38500: LOSS_BATCH = 0.107274, TIME 46615.271492\n",
      "STEP 38550: LOSS_BATCH = 0.110400, TIME 46676.751670\n",
      "STEP 38600: LOSS_BATCH = 0.114060, TIME 46740.378511\n",
      "STEP 38650: LOSS_BATCH = 0.112670, TIME 46803.734748\n",
      "STEP 38700: LOSS_BATCH = 0.111131, TIME 46865.819594\n",
      "STEP 38750: LOSS_BATCH = 0.106337, TIME 46926.182695\n",
      "STEP 38800: LOSS_BATCH = 0.108789, TIME 46989.113645\n",
      "STEP 38850: LOSS_BATCH = 0.106854, TIME 47056.145682\n",
      "STEP 38900: LOSS_BATCH = 0.110909, TIME 47125.614768\n",
      "STEP 38950: LOSS_BATCH = 0.112536, TIME 47199.943828\n",
      "STEP 39000: LOSS_BATCH = 0.110505, TIME 47272.317519\n",
      "STEP 39050: LOSS_BATCH = 0.111913, TIME 47343.154703\n",
      "STEP 39100: LOSS_BATCH = 0.113849, TIME 47412.927066\n",
      "STEP 39150: LOSS_BATCH = 0.109649, TIME 47476.833688\n",
      "STEP 39200: LOSS_BATCH = 0.111610, TIME 47546.898868\n",
      "STEP 39250: LOSS_BATCH = 0.113251, TIME 47616.357335\n",
      "STEP 39300: LOSS_BATCH = 0.112056, TIME 47679.265693\n",
      "STEP 39350: LOSS_BATCH = 0.110500, TIME 47751.372456\n",
      "STEP 39400: LOSS_BATCH = 0.107071, TIME 47820.077163\n",
      "STEP 39450: LOSS_BATCH = 0.109180, TIME 47891.814547\n",
      "STEP 39500: LOSS_BATCH = 0.110627, TIME 47959.447233\n",
      "STEP 39550: LOSS_BATCH = 0.108315, TIME 48020.901633\n",
      "STEP 39600: LOSS_BATCH = 0.110119, TIME 48084.228748\n",
      "STEP 39650: LOSS_BATCH = 0.107429, TIME 48153.160283\n",
      "STEP 39700: LOSS_BATCH = 0.110429, TIME 48230.953529\n",
      "STEP 39750: LOSS_BATCH = 0.111499, TIME 48302.412402\n",
      "STEP 39800: LOSS_BATCH = 0.110932, TIME 48373.964971\n",
      "STEP 39850: LOSS_BATCH = 0.108494, TIME 48449.210085\n",
      "STEP 39900: LOSS_BATCH = 0.113613, TIME 48525.784653\n",
      "STEP 39950: LOSS_BATCH = 0.112923, TIME 48593.948956\n",
      "STEP 40000: LOSS_BATCH = 0.113124, TIME 48660.213462\n",
      "STEP 40050: LOSS_BATCH = 0.110505, TIME 48730.697035\n",
      "STEP 40100: LOSS_BATCH = 0.114741, TIME 48794.303786\n",
      "STEP 40150: LOSS_BATCH = 0.108961, TIME 48862.648499\n",
      "STEP 40200: LOSS_BATCH = 0.109226, TIME 48934.763669\n",
      "STEP 40250: LOSS_BATCH = 0.111964, TIME 49007.532657\n",
      "STEP 40300: LOSS_BATCH = 0.108480, TIME 49078.125358\n",
      "STEP 40350: LOSS_BATCH = 0.106890, TIME 49146.893960\n",
      "STEP 40400: LOSS_BATCH = 0.113442, TIME 49218.104568\n",
      "STEP 40450: LOSS_BATCH = 0.112972, TIME 49293.307285\n",
      "STEP 40500: LOSS_BATCH = 0.108373, TIME 49365.824995\n",
      "STEP 40550: LOSS_BATCH = 0.107910, TIME 49439.248613\n",
      "STEP 40600: LOSS_BATCH = 0.110875, TIME 49508.722119\n",
      "STEP 40650: LOSS_BATCH = 0.113460, TIME 49585.027084\n",
      "STEP 40700: LOSS_BATCH = 0.112438, TIME 49659.455023\n",
      "STEP 40750: LOSS_BATCH = 0.109969, TIME 49734.183674\n",
      "STEP 40800: LOSS_BATCH = 0.110699, TIME 49809.803921\n",
      "STEP 40850: LOSS_BATCH = 0.106746, TIME 49882.348281\n",
      "STEP 40900: LOSS_BATCH = 0.109134, TIME 49956.180110\n",
      "STEP 40950: LOSS_BATCH = 0.113316, TIME 50028.158910\n",
      "STEP 41000: LOSS_BATCH = 0.108678, TIME 50098.976635\n",
      "STEP 41050: LOSS_BATCH = 0.112927, TIME 50170.307654\n",
      "STEP 41100: LOSS_BATCH = 0.110356, TIME 50241.159090\n",
      "STEP 41150: LOSS_BATCH = 0.111794, TIME 50313.975098\n",
      "STEP 41200: LOSS_BATCH = 0.110403, TIME 50387.825122\n",
      "STEP 41250: LOSS_BATCH = 0.109113, TIME 50459.160457\n",
      "STEP 41300: LOSS_BATCH = 0.106153, TIME 50527.696005\n",
      "STEP 41350: LOSS_BATCH = 0.112790, TIME 50593.834424\n",
      "STEP 41400: LOSS_BATCH = 0.110730, TIME 50663.562997\n",
      "STEP 41450: LOSS_BATCH = 0.110682, TIME 50729.212524\n",
      "STEP 41500: LOSS_BATCH = 0.108708, TIME 50793.139436\n",
      "STEP 41550: LOSS_BATCH = 0.114535, TIME 50857.286718\n",
      "STEP 41600: LOSS_BATCH = 0.111388, TIME 50925.011617\n",
      "STEP 41650: LOSS_BATCH = 0.113487, TIME 50995.724219\n",
      "STEP 41700: LOSS_BATCH = 0.111103, TIME 51056.722591\n",
      "STEP 41750: LOSS_BATCH = 0.110056, TIME 51119.294465\n",
      "STEP 41800: LOSS_BATCH = 0.111204, TIME 51181.813836\n",
      "STEP 41850: LOSS_BATCH = 0.110518, TIME 51242.588701\n",
      "STEP 41900: LOSS_BATCH = 0.108321, TIME 51305.463935\n",
      "STEP 41950: LOSS_BATCH = 0.117615, TIME 51370.997566\n",
      "STEP 42000: LOSS_BATCH = 0.108581, TIME 51436.204083\n",
      "STEP 42050: LOSS_BATCH = 0.113896, TIME 51503.286197\n",
      "STEP 42100: LOSS_BATCH = 0.111408, TIME 51572.420027\n",
      "STEP 42150: LOSS_BATCH = 0.114012, TIME 51642.240444\n",
      "STEP 42200: LOSS_BATCH = 0.110402, TIME 51708.960981\n",
      "STEP 42250: LOSS_BATCH = 0.111709, TIME 51774.373085\n",
      "STEP 42300: LOSS_BATCH = 0.114433, TIME 51835.016053\n",
      "STEP 42350: LOSS_BATCH = 0.108185, TIME 51898.559575\n",
      "STEP 42400: LOSS_BATCH = 0.116817, TIME 51972.497942\n",
      "STEP 42450: LOSS_BATCH = 0.113015, TIME 52045.685098\n",
      "STEP 42500: LOSS_BATCH = 0.112792, TIME 52117.298638\n",
      "STEP 42550: LOSS_BATCH = 0.108365, TIME 52194.757655\n",
      "STEP 42600: LOSS_BATCH = 0.107855, TIME 52267.800095\n",
      "STEP 42650: LOSS_BATCH = 0.108580, TIME 52340.223659\n",
      "STEP 42700: LOSS_BATCH = 0.109236, TIME 52414.616713\n",
      "STEP 42750: LOSS_BATCH = 0.109760, TIME 52484.249083\n",
      "STEP 42800: LOSS_BATCH = 0.112210, TIME 52556.369040\n",
      "STEP 42850: LOSS_BATCH = 0.111053, TIME 52631.841270\n",
      "STEP 42900: LOSS_BATCH = 0.110358, TIME 52705.717910\n",
      "STEP 42950: LOSS_BATCH = 0.112952, TIME 52773.413401\n",
      "STEP 43000: LOSS_BATCH = 0.111958, TIME 52840.402016\n",
      "STEP 43050: LOSS_BATCH = 0.112307, TIME 52909.559068\n",
      "STEP 43100: LOSS_BATCH = 0.114988, TIME 52975.690649\n",
      "STEP 43150: LOSS_BATCH = 0.113709, TIME 53044.507656\n",
      "STEP 43200: LOSS_BATCH = 0.111222, TIME 53109.337565\n",
      "STEP 43250: LOSS_BATCH = 0.112891, TIME 53172.842363\n",
      "STEP 43300: LOSS_BATCH = 0.110649, TIME 53241.786001\n",
      "STEP 43350: LOSS_BATCH = 0.110001, TIME 53311.430147\n",
      "STEP 43400: LOSS_BATCH = 0.111199, TIME 53378.052819\n",
      "STEP 43450: LOSS_BATCH = 0.109705, TIME 53439.573874\n",
      "STEP 43500: LOSS_BATCH = 0.109980, TIME 53500.590831\n",
      "STEP 43550: LOSS_BATCH = 0.109325, TIME 53561.001817\n",
      "STEP 43600: LOSS_BATCH = 0.110744, TIME 53623.780451\n",
      "STEP 43650: LOSS_BATCH = 0.108030, TIME 53687.554887\n",
      "STEP 43700: LOSS_BATCH = 0.111885, TIME 53749.430411\n",
      "STEP 43750: LOSS_BATCH = 0.111305, TIME 53813.023248\n",
      "STEP 43800: LOSS_BATCH = 0.110360, TIME 53876.948820\n",
      "STEP 43850: LOSS_BATCH = 0.110450, TIME 53938.828892\n",
      "STEP 43900: LOSS_BATCH = 0.108336, TIME 54006.004836\n",
      "STEP 43950: LOSS_BATCH = 0.106622, TIME 54070.590079\n",
      "STEP 44000: LOSS_BATCH = 0.110094, TIME 54147.815754\n",
      "STEP 44050: LOSS_BATCH = 0.108423, TIME 54217.238914\n",
      "STEP 44100: LOSS_BATCH = 0.110139, TIME 54282.960259\n",
      "STEP 44150: LOSS_BATCH = 0.113128, TIME 54351.552081\n",
      "STEP 44200: LOSS_BATCH = 0.107843, TIME 54423.832537\n",
      "STEP 44250: LOSS_BATCH = 0.111860, TIME 54495.199569\n",
      "STEP 44300: LOSS_BATCH = 0.110922, TIME 54562.309894\n",
      "STEP 44350: LOSS_BATCH = 0.113759, TIME 54634.183179\n",
      "STEP 44400: LOSS_BATCH = 0.112076, TIME 54699.387444\n",
      "STEP 44450: LOSS_BATCH = 0.111894, TIME 54769.846239\n",
      "STEP 44500: LOSS_BATCH = 0.113341, TIME 54845.780411\n",
      "STEP 44550: LOSS_BATCH = 0.109280, TIME 54916.050034\n",
      "STEP 44600: LOSS_BATCH = 0.107078, TIME 54980.437755\n",
      "STEP 44650: LOSS_BATCH = 0.109258, TIME 55041.147916\n",
      "STEP 44700: LOSS_BATCH = 0.117795, TIME 55106.562885\n",
      "STEP 44750: LOSS_BATCH = 0.111912, TIME 55176.484488\n",
      "STEP 44800: LOSS_BATCH = 0.107170, TIME 55242.289636\n",
      "STEP 44850: LOSS_BATCH = 0.109344, TIME 55314.717317\n",
      "STEP 44900: LOSS_BATCH = 0.112970, TIME 55383.091150\n",
      "STEP 44950: LOSS_BATCH = 0.110823, TIME 55448.682989\n",
      "STEP 45000: LOSS_BATCH = 0.111386, TIME 55514.902010\n",
      "STEP 45050: LOSS_BATCH = 0.108216, TIME 55582.635300\n",
      "STEP 45100: LOSS_BATCH = 0.110937, TIME 55648.431876\n",
      "STEP 45150: LOSS_BATCH = 0.111322, TIME 55718.312850\n",
      "STEP 45200: LOSS_BATCH = 0.118358, TIME 55789.463148\n",
      "STEP 45250: LOSS_BATCH = 0.111598, TIME 55858.814342\n",
      "STEP 45300: LOSS_BATCH = 0.114283, TIME 55927.887437\n",
      "STEP 45350: LOSS_BATCH = 0.110125, TIME 55996.008948\n",
      "STEP 45400: LOSS_BATCH = 0.116865, TIME 56069.426363\n",
      "STEP 45450: LOSS_BATCH = 0.111323, TIME 56142.417339\n",
      "STEP 45500: LOSS_BATCH = 0.110081, TIME 56212.424460\n",
      "STEP 45550: LOSS_BATCH = 0.108702, TIME 56277.798912\n",
      "STEP 45600: LOSS_BATCH = 0.111109, TIME 56347.424987\n",
      "STEP 45650: LOSS_BATCH = 0.107964, TIME 56421.276788\n",
      "STEP 45700: LOSS_BATCH = 0.115607, TIME 56497.683768\n",
      "STEP 45750: LOSS_BATCH = 0.114745, TIME 56567.891542\n",
      "STEP 45800: LOSS_BATCH = 0.106581, TIME 56637.239315\n",
      "STEP 45850: LOSS_BATCH = 0.111697, TIME 56707.389627\n",
      "STEP 45900: LOSS_BATCH = 0.113605, TIME 56775.807885\n",
      "STEP 45950: LOSS_BATCH = 0.108039, TIME 56845.325686\n",
      "STEP 46000: LOSS_BATCH = 0.108717, TIME 56913.309010\n",
      "STEP 46050: LOSS_BATCH = 0.107746, TIME 56981.433317\n",
      "STEP 46100: LOSS_BATCH = 0.107783, TIME 57047.414719\n",
      "STEP 46150: LOSS_BATCH = 0.112082, TIME 57112.022968\n",
      "STEP 46200: LOSS_BATCH = 0.112367, TIME 57173.394531\n",
      "STEP 46250: LOSS_BATCH = 0.112658, TIME 57234.800098\n",
      "STEP 46300: LOSS_BATCH = 0.111587, TIME 57295.545578\n",
      "STEP 46350: LOSS_BATCH = 0.111778, TIME 57356.014987\n",
      "STEP 46400: LOSS_BATCH = 0.111610, TIME 57419.367878\n",
      "STEP 46450: LOSS_BATCH = 0.110139, TIME 57485.412639\n",
      "STEP 46500: LOSS_BATCH = 0.106928, TIME 57559.833756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8f90e3d89701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STEP %d: LOSS_BATCH = %f, TIME %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_train_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1384\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1385\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m         clear_devices=clear_devices)\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1642\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m       \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[0;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopying\u001b[0m \u001b[0meach\u001b[0m \u001b[0mindividual\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mextend\u001b[0;34m(self, elem_seq)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mnew_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;31m# self._message_listener.Modified() not required here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;31m# mutations to submessages already propagate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/message.py\u001b[0m in \u001b[0;36mCopyFrom\u001b[0;34m(self, other_msg)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mother_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36m_Clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bruno/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mModified\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener_for_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c = np.inf\n",
    "#run 1 37303\n",
    "#run 2 100000\n",
    "\n",
    "#run 3 sintel 46500\n",
    "for i in range(100000):\n",
    "    batch = generate_batch(_dataset,_batch_size)\n",
    "    feed = {x: batch[0], x_gt:batch[1]}\n",
    "    summary,_ = sess.run([merged,optimizer], feed_dict=feed)\n",
    "    if i % _step_test == 0 and i>0:\n",
    "        \n",
    "        c_old = c\n",
    "        c = sess.run(cost, feed_dict=feed)\n",
    "        print(\"STEP %d: LOSS_BATCH = %f, TIME %f\"%(i,c,time.time()-start))\n",
    "        save_path = saver.save(sess, _train_folder+\"/model.ckpt\")\n",
    "        writer.add_summary(summary, i)\n",
    "    \n",
    "    if i % _step_viz == 0 and i>0:\n",
    "        test(i/_step_viz)\n",
    "        \n",
    "print(\"Optimization Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
